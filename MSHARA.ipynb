{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uriFpj1P4bMQ",
        "outputId": "0904f6c3-5964-4999-93dc-7016d306cd4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVdchlDB8j59",
        "outputId": "70ea6d12-facb-49ea-b0b4-e567122ebf11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/carknows\n",
            "Cardataset_path.ipynb  \u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mFineGym\u001b[0m/  \u001b[01;34mresults\u001b[0m/  \u001b[01;34mrun\u001b[0m/  transforms.py\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/carknows\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVUJoTfdsWif"
      },
      "outputs": [],
      "source": [
        "# !mkdir FineGym\n",
        "# !unzip /content/drive/MyDrive/carknows/data/FineGym.zip -d /content/FineGym"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build local file system on Colab"
      ],
      "metadata": {
        "id": "_hOftENttXJ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QHLF-h0CKTC"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/Brain4Cars\n",
        "# # !cd /content/Brain4Cars\n",
        "# # !mkdir Data Lab\n",
        "!mkdir /content/Brain4Cars/Data \n",
        "!mkdir /content/Brain4Cars/lab\n",
        "!unzip /content/drive/MyDrive/carknows/data/Brain4Car/clip_lab.zip -d /content/Brain4Cars/lab\n",
        "!unzip /content/drive/MyDrive/carknows/data/Brain4Car/face.zip -d /content/Brain4Cars/Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtciy1N-5v0y"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/carknows/run/run_10/models/Jan15_08-45-39_eedb267c90f3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r3_6vgg7xPV"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr4PRlBOoEwu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import scipy.io as scio\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "\n",
        "class Path(object):\n",
        "    @staticmethod\n",
        "    def db_dir(database):\n",
        "        if database == 'face':\n",
        "            # folder that contains class labels\n",
        "            root_dir = '/content/Brain4Cars/Data'\n",
        "\n",
        "            flow_dir = '/content/Brain4Cars/Data'\n",
        "\n",
        "            # Save preprocess data into output_dir\n",
        "            output_dir = '/content/Brain4Cars/Data'\n",
        "\n",
        "            seq_lab_dir = '/content/Brain4Cars/lab/clip_lab'\n",
        "\n",
        "        elif database == 'gtea':\n",
        "\n",
        "            root_dir = '/content/drive/MyDrive/carknows/data/GTEA/GTEA_dt'\n",
        "\n",
        "            flow_dir = '/content/drive/MyDrive/carknows/data/GTEA/GTEA_dt'\n",
        "\n",
        "            # Save preprocess data into output_dir\n",
        "            output_dir = '/content/drive/MyDrive/carknows/data/GTEA/GTEA_dt'\n",
        "\n",
        "            seq_lab_dir = '/content/drive/MyDrive/carknows/data/GTEA/GTEA_lab'\n",
        "\n",
        "        elif database == 'finegym':\n",
        "\n",
        "            root_dir = '/content/FineGym/finegym_data'\n",
        "\n",
        "            flow_dir = '/content/FineGym/finegym_data'\n",
        "\n",
        "            # Save preprocess data into output_dir\n",
        "            output_dir = '/content/FineGym/finegym_data'\n",
        "\n",
        "            seq_lab_dir = '/content/FineGym/finegym_label'\n",
        "\n",
        "        return root_dir, output_dir, seq_lab_dir, flow_dir\n",
        "\n",
        "\n",
        "class CarDataset_multi_2(Dataset):\n",
        "    def __init__(self, dataset='face', split='val', clip_len=16, transform1=None, transform2=None):\n",
        "        self.root_dir, self.output_dir, self.seq_lab_dir, self.flow_dir = Path.db_dir(dataset)\n",
        "        folder = os.path.join(self.output_dir, split)\n",
        "        print('folder:', folder)\n",
        "        seqlab_folder = os.path.join(self.seq_lab_dir, split)\n",
        "\n",
        "        self.clip_len = clip_len\n",
        "        self.split = split\n",
        "\n",
        "        self.fnames, labels, self.flabels = [], [], []\n",
        "\n",
        "        self.resize_height = 128\n",
        "        self.resize_width = 128\n",
        "        self.crop_size = 112\n",
        "        self.dataset = dataset\n",
        "\n",
        "        if dataset =='gtea':\n",
        "            self.resize_height_resnet = 200\n",
        "            self.resize_width_resnet = 320\n",
        "        elif dataset == 'face':\n",
        "            self.resize_height_resnet = 224\n",
        "            self.resize_width_resnet = 224\n",
        "        elif dataset == 'finegym':\n",
        "            self.resize_height_resnet = 180\n",
        "            self.resize_width_resnet = 320\n",
        "\n",
        "        self.transform1 = transform1\n",
        "        self.transform2 = transform2\n",
        "\n",
        "        for label in sorted(os.listdir(folder)):\n",
        "            for fname in os.listdir(os.path.join(folder, label)):\n",
        "                self.fnames.append(os.path.join(folder, label, fname))\n",
        "                self.flabels.append(os.path.join(seqlab_folder, label, fname))\n",
        "\n",
        "                labels.append(label)\n",
        "        assert len(labels) == len(self.fnames)\n",
        "        print('Number of {} videos: {:d}'.format(split, len(self.fnames)))\n",
        "\n",
        "        # Prepare a mapping between the label names (strings) and indices (ints)\n",
        "        self.label2index = {label: index for index, label in enumerate(sorted(set(labels)))}\n",
        "        # Convert the list of label names into an array of label indices\n",
        "        self.label_array = np.array([self.label2index[label] for label in labels], dtype=int)\n",
        "\n",
        "    def __len__(self):\n",
        "        print('len')\n",
        "        return len(self.fnames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        buffer, buffer_resnet = self.load_frames(self.fnames[index])\n",
        "        buffer, buffer_resnet = self.crop(buffer, buffer_resnet, self.clip_len, self.crop_size)\n",
        "        labels = np.array(self.label_array[index])\n",
        "        seq_labels = self.get_seq_labels_from_mat(self.flabels[index], self.dataset)\n",
        "\n",
        "        buffer, buffer_resnet = self.to_tensor(buffer, buffer_resnet, self.transform1, self.transform2)\n",
        "\n",
        "        return buffer, buffer_resnet, torch.from_numpy(labels), seq_labels\n",
        "\n",
        "    def load_frames(self, file_dir):\n",
        "        frames = sorted([os.path.join(file_dir, img) for img in os.listdir(file_dir)])\n",
        "        frame_count = len(frames)\n",
        "\n",
        "        buffer = np.empty((frame_count, self.resize_height, self.resize_width, 3), np.dtype('float32'))\n",
        "        buffer_resnet = np.empty((frame_count, self.resize_height_resnet, self.resize_width_resnet, 3),\n",
        "                                 np.dtype('float32'))\n",
        "        resize = (self.resize_height, self.resize_width)\n",
        "        for i, frame_name in enumerate(frames):\n",
        "\n",
        "            frame_pil = Image.open(frame_name)\n",
        "            frame_pil_re = frame_pil.resize((self.resize_height, self.resize_width))\n",
        "\n",
        "            buffer[i] = frame_pil_re\n",
        "            buffer_resnet[i] = frame_pil\n",
        "\n",
        "        return buffer, buffer_resnet\n",
        "\n",
        "\n",
        "    def to_tensor(self, buffer, buffer_resnet, use_transform1, use_transform2):\n",
        "        buffer1 = torch.from_numpy(buffer) / 255\n",
        "\n",
        "        if use_transform1 is not None:\n",
        "            buffer = use_transform1(buffer1)\n",
        "        else:\n",
        "            buffer = buffer.transpose((3, 0, 1, 2)) \n",
        "            buffer = torch.from_numpy(buffer)\n",
        "\n",
        "        if use_transform2 is not None:\n",
        "            buffer_resnet = use_transform2(buffer_resnet)\n",
        "        else:\n",
        "            buffer_resnet = buffer_resnet.transpose((0, 3, 1, 2))  # used for C3D\n",
        "            buffer_resnet = torch.from_numpy(buffer_resnet)\n",
        "\n",
        "        return buffer, buffer_resnet\n",
        "\n",
        "\n",
        "    def crop(self, buffer, buffer_resnet, clip_len, crop_size):\n",
        "        pred_con = 5\n",
        "\n",
        "        bound_sep = 5\n",
        "\n",
        "        if clip_len > 120:\n",
        "            split_index = np.linspace(0, clip_len, clip_len)\n",
        "            split_index = np.ceil(split_index)\n",
        "            split_index = split_index.astype('int64')\n",
        "        else:\n",
        "            bound = np.rint(buffer.shape[0] / bound_sep)\n",
        "\n",
        "            begin_index = np.random.randint(0, bound)\n",
        "            upper_bound = buffer.shape[0] - pred_con\n",
        "            end_index = np.random.randint(bound * (bound_sep - 1), upper_bound)\n",
        "            split_index = np.linspace(begin_index, end_index, clip_len)\n",
        "            split_index = np.ceil(split_index)\n",
        "            split_index = split_index.astype('int64')\n",
        "\n",
        "        # Randomly select start indices in order to crop the video\n",
        "        height_index = np.random.randint(buffer.shape[1] - crop_size)\n",
        "        width_index = np.random.randint(buffer.shape[2] - crop_size)\n",
        "\n",
        "        buffer = buffer[split_index,\n",
        "                 height_index:height_index + crop_size,\n",
        "                 width_index:width_index + crop_size, :]\n",
        "\n",
        "        buffer_resnet = buffer_resnet[split_index, :, :, :]\n",
        "\n",
        "        return buffer, buffer_resnet\n",
        "\n",
        "    def get_seq_labels_from_mat(self, flabels, dataset):\n",
        "        if dataset == 'face':\n",
        "            mat_path = os.path.join(flabels, 'lab.mat')\n",
        "            mat_lab = scio.loadmat(mat_path)\n",
        "            mat_lab_seq = mat_lab['lab_gt']\n",
        "            seq_lab = mat_lab_seq.astype(\"int32\")\n",
        "        elif dataset == 'gtea':\n",
        "            mat_path = os.path.join(flabels, 'state.mat')\n",
        "            mat_lab = scio.loadmat(mat_path)\n",
        "            mat_lab_seq = mat_lab['state_p']\n",
        "            seq_lab = mat_lab_seq.astype(\"int32\")\n",
        "        elif dataset == 'finegym':\n",
        "            mat_path = os.path.join(flabels, 'state.mat')\n",
        "            mat_lab = scio.loadmat(mat_path)\n",
        "            mat_lab_seq = mat_lab['state']\n",
        "            seq_lab = mat_lab_seq.astype(\"int32\")\n",
        "            \n",
        "        # identify if any folders have incomplete data\n",
        "        if seq_lab.shape[1] < 151:\n",
        "            print('len_seq_lab', seq_lab.shape, seq_lab)\n",
        "            print('mat_path', mat_path)\n",
        "        return seq_lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLtChaLvoydb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "print('main')\n",
        "dataset = 'face'\n",
        "clip_len = 150\n",
        "\n",
        "class ConvertBHWCtoBCHW(nn.Module):\n",
        "    \"\"\"Convert tensor from (B, H, W, C) to (B, C, H, W)\n",
        "    \"\"\"\n",
        "    def forward(self, vid: torch.Tensor) -> torch.Tensor:\n",
        "        return vid.permute(0, 3, 1, 2)\n",
        "\n",
        "class ConvertBCHWtoCBHW(nn.Module):\n",
        "    \"\"\"Convert tensor from (B, C, H, W) to (C, B, H, W)\n",
        "    \"\"\"\n",
        "    def forward(self, vid: torch.Tensor) -> torch.Tensor:\n",
        "        return vid.permute(1, 0, 2, 3)\n",
        "\n",
        "mean = [0.43216, 0.394666, 0.37645]\n",
        "std = [0.22803, 0.22145, 0.216989]\n",
        "\n",
        "resize_size = 224\n",
        "crop_size = 112\n",
        "\n",
        "mean_res = [0.485, 0.456, 0.406]\n",
        "std_res = [0.229, 0.224, 0.225]\n",
        "\n",
        "trans1 = [\n",
        "    ConvertBHWCtoBCHW(),\n",
        "    transforms.ConvertImageDtype(torch.float32),\n",
        "]\n",
        "trans1.extend([\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "    ConvertBCHWtoCBHW()])\n",
        "\n",
        "trans2 = [\n",
        "    ConvertBHWCtoBCHW(),\n",
        "    transforms.ConvertImageDtype(torch.float32),\n",
        "]\n",
        "trans2.extend([\n",
        "    transforms.Normalize(mean=mean_res, std=std_res)])\n",
        "\n",
        "transform1_t = transforms.Compose(trans1)\n",
        "transform2_t = transforms.Compose(trans2)\n",
        "\n",
        "train_dataloader = DataLoader(CarDataset_multi_2(dataset=dataset, split='train', clip_len=clip_len,\n",
        "                                              transform1=transform1_t, transform2=None),\n",
        "                              batch_size=3, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "val_dataloader = DataLoader(CarDataset_multi_2(dataset=dataset, split='val', clip_len=clip_len,\n",
        "                                            transform1=transform1_t, transform2=None),\n",
        "                            batch_size=3, num_workers=4, pin_memory=True)\n",
        "\n",
        "test_dataloader = DataLoader(CarDataset_multi_2(dataset=dataset, split='test', clip_len=clip_len,\n",
        "                                              transform1=transform1_t, transform2=None),\n",
        "                              batch_size=3, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "for i, sample in enumerate(train_dataloader):\n",
        "    inputs1 = sample[0]\n",
        "    inputs2 = sample[1]\n",
        "    labels = sample[2]\n",
        "    labels_seq = sample[3]\n",
        "    print('main input 1', inputs1.size())\n",
        "    print('main input 2', inputs2.size())\n",
        "    print('main label', labels, labels.size())\n",
        "    print('main label seq', labels_seq.size())\n",
        "\n",
        "    if i == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM_ANNO: extract high-level temporal features from each clip.\n",
        "\n",
        "Fusion_net_2: fuse features from the 3d and 2d branches.\n",
        "\n",
        "Prediction_net: fuse two-branch features and predict the activities for next clip. \n"
      ],
      "metadata": {
        "id": "LfYYk_2xumAT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meyyN8YQedZc"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timeit\n",
        "from datetime import datetime\n",
        "import socket\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class LSTM_ANNO(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(LSTM_ANNO, self).__init__()\n",
        "        # defining encoder LSTM layers\n",
        "        feat_dim = 512\n",
        "        self.clip_len = 16\n",
        "        self.lstm_hidden_size = 512\n",
        "        self.num_classes = num_classes\n",
        "        self.gru1 = nn.GRU(feat_dim, feat_dim//2, 2, batch_first=True, bidirectional=True)   # Fusion net: 1152, Concate: 1024, C3D: 4096, RES3D: 512\n",
        "        self.gru2 = nn.GRU(feat_dim, feat_dim//2, num_layers=2, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(feat_dim, feat_dim//4)\n",
        "        # self.bn1 = nn.BatchNorm1d(512, momentum=0.01)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc_final_score = nn.Linear(feat_dim//4, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state = None\n",
        "        gru_output, (h_n) = self.gru1(x, state)\n",
        "        gru1_output = gru_output.clone()\n",
        "        gru_output, (h_n) = self.gru2(gru_output)\n",
        "        gru_output = self.relu(self.fc1(gru_output[:, -1, :]))\n",
        "        final_score = self.fc_final_score(gru_output)\n",
        "\n",
        "        return final_score, gru1_output\n",
        "\n",
        "\n",
        "def basic_multi_fusion(spat_feat, temp_feat):\n",
        "    return spat_feat*temp_feat\n",
        "\n",
        "\n",
        "def basic_sum_fusion(spat_feat, temp_feat):\n",
        "    return spat_feat+temp_feat\n",
        "\n",
        "\n",
        "def basic_max_fusion(spat_feat, temp_feat):\n",
        "    return torch.max(spat_feat, temp_feat)\n",
        "\n",
        "\n",
        "def basic_concate_fusion(spat_feat, temp_feat):\n",
        "    return torch.cat((spat_feat, temp_feat), 1)\n",
        "\n",
        "\n",
        "class basic_conv_fusion(nn.Module):\n",
        "    def __init__(self, pred_horizon):\n",
        "        super(basic_conv_fusion, self).__init__()\n",
        "        self.conv1d = nn.Conv1d(pred_horizon*2, 256, 3, stride=1)\n",
        "        self.conv1d_2 = nn.Conv1d(2, 16, 3, stride=3)\n",
        "\n",
        "    def forward(self, spat_feat, temp_feat):\n",
        "        if len(spat_feat.size()) > 2:\n",
        "            x = torch.cat((temp_feat, spat_feat), 1)\n",
        "            x = self.conv1d(x)\n",
        "        else:\n",
        "            x = torch.stack((temp_feat, spat_feat), 1)\n",
        "            x = self.conv1d_2(x)\n",
        "            x = x.view(spat_feat.size()[0], -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class basic_conv_demen1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(basic_conv_demen1, self).__init__()\n",
        "        # self.c1 = nn.Conv3d(in_channels=64, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.c2 = nn.Conv3d(in_channels=16, out_channels=4, kernel_size=3, padding=1)\n",
        "        self.c3 = nn.Conv3d(in_channels=4, out_channels=1, kernel_size=3, padding=1)\n",
        "\n",
        "        self.c4 = nn.Conv3d(in_channels=16, out_channels=1, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x.permute((0, 2, 1, 3, 4))\n",
        "        # x = self.c1(x)\n",
        "        x1 = self.c2(x0)\n",
        "        x1 = self.c3(x1)\n",
        "\n",
        "        x2 = self.c4(x0)\n",
        "        x2 = x2.squeeze(1)\n",
        "        print('x2.size', x2.size())\n",
        "\n",
        "        x1 = x1.squeeze(1)\n",
        "\n",
        "        x = x1+x2\n",
        "        return x\n",
        "\n",
        "\n",
        "class Fusion_net_2(nn.Module):\n",
        "    def __init__(self, num_classes=4, fusion_ind=True, pred_horizon=8):\n",
        "        super(Fusion_net_2, self).__init__()\n",
        "\n",
        "        self.fc_hidden1 = 512          # 512， 1024, 2720(conv1d)\n",
        "        self.fc_hidden2 = 512\n",
        "        self.reduction_ratio = 2\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.fc1_act = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
        "        self.fc2_act = nn.Linear(self.fc_hidden2, self.num_classes)\n",
        "\n",
        "        self.fc1_int = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.spat_weigh = nn.Parameter(torch.tensor([1.0]))\n",
        "        self.temp_weigh = nn.Parameter(torch.tensor([1.0]))\n",
        "\n",
        "        self.fusion_ind = fusion_ind\n",
        "        self.convfusion = basic_conv_fusion(pred_horizon=pred_horizon)\n",
        "\n",
        "    def forward(self, x_temp, x_spat):\n",
        "\n",
        "        # if use bounded fusion method\n",
        "\n",
        "        # if self.fusion_ind:\n",
        "        #     self.temp_weigh = nn.Parameter(1 - self.spat_weigh)\n",
        "        #     spat_weigh = self.spat_weigh.clone()\n",
        "        #     temp_weigh = self.temp_weigh.clone()\n",
        "\n",
        "        #     spat_weigh = spat_weigh.clamp(0, 1)\n",
        "        #     temp_weigh = temp_weigh.clamp(0, 1)\n",
        "\n",
        "        #     x_temp = temp_weigh * x_temp\n",
        "        #     x_spat = spat_weigh * x_spat\n",
        "        #     # print('x_temp param', temp_weigh, 'x_spat param', spat_weigh)\n",
        "\n",
        "        x_temp = self.temp_weigh * x_temp\n",
        "        x_spat = self.spat_weigh * x_spat\n",
        "\n",
        "        # x_fuse = self.convfusion(x_spat, x_temp)\n",
        "        x_fuse = basic_multi_fusion(x_spat, x_temp)\n",
        "\n",
        "        x0 = self.fc1_int(x_fuse)\n",
        "        x0 = x0.unsqueeze(0)\n",
        "        x0 = x0.transpose(0, 1)\n",
        "\n",
        "        x1 = self.relu(self.fc1_act(x_fuse))\n",
        "        logits = self.fc2_act(x1)\n",
        "\n",
        "        x1 = x1.unsqueeze(0)\n",
        "        x1 = x1.transpose(0, 1)\n",
        "\n",
        "        return x0, logits, x1, [self.spat_weigh, self.temp_weigh]\n",
        "\n",
        "    def get_1x_lr_params(model):\n",
        "        \"\"\"\n",
        "        This generator returns all the parameters for conv and two fc layers of the net.\n",
        "        \"\"\"\n",
        "        b = [model.fc1_act, model.fc2_act, model.fc1_int, model.convfusion]\n",
        "        for i in range(len(b)):\n",
        "            for k in b[i].parameters():\n",
        "                if k.requires_grad:\n",
        "                    yield k\n",
        "\n",
        "\n",
        "class prediction_net(nn.Module):\n",
        "    def __init__(self, num_activity_classes=4, num_intent_classes=5, fusion_ind=True, pred_horizon=7):\n",
        "        super(prediction_net, self).__init__()\n",
        "        self.num_activity_classes = num_activity_classes\n",
        "        self.num_intent_classes = num_intent_classes\n",
        "\n",
        "        feat_dim = 512\n",
        "        fused_dim = 512\n",
        "\n",
        "        self.gru1 = nn.GRU(feat_dim, feat_dim//2, 2, batch_first=True, bidirectional=True)   # Fusion net: 1152, Concate: 1024, C3D: 4096, RES3D: 512\n",
        "        self.gru2 = nn.GRU(feat_dim, feat_dim//2, 2, batch_first=True, bidirectional=True)   # Fusion net: 1152, Concate: 1024, C3D: 4096, RES3D: 512\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(fused_dim, feat_dim//2)\n",
        "        self.fc2 = nn.Linear(fused_dim, feat_dim//2)\n",
        "        self.fc_final_intent_score = nn.Linear(feat_dim//2, self.num_intent_classes)\n",
        "        self.fc_final_activity_score = nn.Linear(feat_dim//2, self.num_activity_classes)\n",
        "\n",
        "        self.intent_weigh = nn.Parameter(torch.tensor([1.0]))\n",
        "        self.act_weigh = nn.Parameter(torch.tensor([1.0]))\n",
        "        # fusion_ind = False\n",
        "        self.fusion_ind = fusion_ind\n",
        "        self.convfusion = basic_conv_fusion(pred_horizon=pred_horizon)\n",
        "\n",
        "    def forward(self, x_activity_feats, x_gru_out):\n",
        "        state = None\n",
        "        gru_output_activity, (h_n) = self.gru1(x_activity_feats)\n",
        "        gru_output_activity, (h_n) = self.gru2(gru_output_activity)\n",
        "        \n",
        "        # uncomment if use bounded fusion method\n",
        "        # if self.fusion_ind:\n",
        "        #     self.act_weigh = nn.Parameter(1 - self.intent_weigh)\n",
        "\n",
        "        #     intent_weigh = self.intent_weigh.clone()\n",
        "        #     act_weigh = self.act_weigh.clone()\n",
        "\n",
        "        #     intent_weigh = intent_weigh.clamp(0, 1)\n",
        "        #     act_weigh = act_weigh.clamp(0, 1)\n",
        "        #     # print('self.temp_weigh', self.temp_weigh)\n",
        "\n",
        "        #     x_pred_sub_act = (act_weigh * gru_output_activity)\n",
        "        #     x_pred_sub_int = (intent_weigh * x_gru_out)\n",
        "\n",
        "        #     print('activity param', act_weigh, 'intent param', intent_weigh)\n",
        "        # else:\n",
        "        #     x_pred_sub_act = gru_output_activity\n",
        "        #     x_pred_sub_int = x_gru_out\n",
        "\n",
        "        x_pred_sub_act = self.act_weigh * gru_output_activity\n",
        "        x_pred_sub_int = self.intent_weigh * x_gru_out\n",
        "\n",
        "        # gru_output_fuse = self.convfusion(x_pred_sub_act, x_pred_sub_int)\n",
        "        gru_output_fuse = basic_multi_fusion(x_pred_sub_act, x_pred_sub_int)\n",
        "\n",
        "        gru_output1 = self.relu(self.fc1(gru_output_fuse[:, -1, :]))\n",
        "        gru_output2 = self.relu(self.fc2(gru_output_fuse[:, -1, :]))\n",
        "\n",
        "        final_intent_score = self.fc_final_intent_score(gru_output2)\n",
        "        final_activity_score = self.fc_final_activity_score(gru_output1)\n",
        "\n",
        "        return final_intent_score, final_activity_score, [self.intent_weigh, self.act_weigh]\n",
        "\n",
        "    def get_1x_lr_params(model):\n",
        "        \"\"\"\n",
        "        This generator returns all the parameters for conv and two fc layers of the net.\n",
        "        \"\"\"\n",
        "        b = [model.gru1, model.gru2, model.fc1, model.fc2, model.fc_final_intent_score, model.fc_final_activity_score,\n",
        "            model.convfusion]\n",
        "\n",
        "        for i in range(len(b)):\n",
        "            for k in b[i].parameters():\n",
        "                if k.requires_grad:\n",
        "                    yield k\n",
        "\n",
        "\n",
        "inputs1 = torch.rand(3, 8, 512)\n",
        "inputs2 = torch.rand(3, 8, 512)\n",
        "net = prediction_net(num_activity_classes=4, num_intent_classes=5,pred_horizon=8)\n",
        "output1, output2, [int_weigh, act_weigh] = net.forward(inputs1, inputs2)\n",
        "\n",
        "print('outputs size:', output1.size(), output2.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initiate either 3D Resnet-18 or R(2+1)D backbone"
      ],
      "metadata": {
        "id": "DBsZlwwouV6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "06f93c06a50241bf93efa790e26d7f7b",
            "0075077355bb456f95c07ac181ce5825",
            "faef3691c80748708915508c5de84068",
            "626fd57856624c04af4f04c0f2d16667",
            "e2e343d7ca12474a8cb130e70503da5a",
            "48528cd23f684381ab922e8032c1cdb4",
            "42f97674d39642358e9d04e36b5e3d0a",
            "8757a349f202487198b32b792f8468a8",
            "f9e2f2ec239b41f2bc2b0b2f88a39a1b",
            "6b7b1cff570a4d159f149ea955fe651c",
            "4ce4eab49b7c430294ead3c30e32e2cc",
            "534138a928d245df941cddd1aad8d064",
            "cc73b26bc04f44cdb5fdeae1097a5879",
            "fb90682952df4523ba081ef4f9e2553b",
            "1f7616eaf68d496f9acdb382e3a5152a",
            "ca69bd5f6855439ca241ae10f94dc0f1",
            "8f2d2a6ccbd4425681dfbc59b8b2026b",
            "c30d0c37dcf64ac2b39086118da1289c",
            "d10a37f319bd48da81d59afa60db35e9",
            "e4686fff6cfb44d0ae3e936355021420",
            "e304106727244844af3fba68933e1b7e",
            "a5e3ce0206ff403f864889b2cfc2e24c"
          ]
        },
        "id": "Tv4gaiAjiyZy",
        "outputId": "b60badbe-7d24-4c1a-bc3c-f20d46c0c106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth\" to /root/.cache/torch/hub/checkpoints/r2plus1d_18-91a641e6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/120M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06f93c06a50241bf93efa790e26d7f7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "534138a928d245df941cddd1aad8d064"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output 3d size: torch.Size([2, 512]) output 2d size: torch.Size([2, 512])\n",
            "Trainable Parameters: 42.477M\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "import numpy as np\n",
        "\n",
        "# __all__ = ['r3d_18', 'ResNet2d', 'resnet18']\n",
        "\n",
        "model_urls = {\n",
        "    'r3d_18': 'https://download.pytorch.org/models/r3d_18-b3b3357e.pth',\n",
        "    'r2plus1d_18': 'https://download.pytorch.org/models/r2plus1d_18-91a641e6.pth',\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n",
        "}\n",
        "\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class ChannelGate_layer1(nn.Module):\n",
        "    def __init__(self, pool_types=['avg', 'max']):\n",
        "        super(ChannelGate_layer1, self).__init__()\n",
        "        self.gate_channels = 16\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.gate_channels, self.gate_channels // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.gate_channels // 2, self.gate_channels)\n",
        "            )\n",
        "        self.pool_types = pool_types\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute((0, 2, 1, 3, 4))\n",
        "        channel_att_sum = True\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type == 'avg':\n",
        "                avg_pool = nn.AvgPool3d((x.size(2), x.size(3), x.size(4)), stride=(x.size(2), x.size(3), x.size(4)))\n",
        "                x_mid = avg_pool(x)\n",
        "                channel_att_raw = self.mlp(x_mid)\n",
        "            elif pool_type == 'max':\n",
        "                max_pool = nn.MaxPool3d((x.size(2), x.size(3), x.size(4)), stride=(x.size(2), x.size(3), x.size(4)))\n",
        "                x_mid = max_pool(x)\n",
        "                channel_att_raw = self.mlp(x_mid)\n",
        "\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = torch.sigmoid(channel_att_raw).unsqueeze(2).unsqueeze(3).unsqueeze(4).expand_as(x)\n",
        "        return x * scale\n",
        "\n",
        "\n",
        "class ChannelGate_layer2(nn.Module):\n",
        "    def __init__(self, pool_types=['avg', 'max']):\n",
        "        super(ChannelGate_layer2, self).__init__()\n",
        "        self.gate_channels = 8\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.gate_channels, self.gate_channels // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.gate_channels // 2, self.gate_channels)\n",
        "        )\n",
        "        self.pool_types = pool_types\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute((0, 2, 1, 3, 4))\n",
        "        channel_att_sum = True\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type == 'avg':\n",
        "                avg_pool = nn.AvgPool3d((x.size(2), x.size(3), x.size(4)), stride=(x.size(2), x.size(3), x.size(4)))\n",
        "                x_mid = avg_pool(x)\n",
        "                channel_att_raw = self.mlp(x_mid)\n",
        "            elif pool_type == 'max':\n",
        "                max_pool = nn.MaxPool3d((x.size(2), x.size(3), x.size(4)), stride=(x.size(2), x.size(3), x.size(4)))\n",
        "                x_mid = max_pool(x)\n",
        "                channel_att_raw = self.mlp(x_mid)\n",
        "\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = torch.sigmoid(channel_att_raw).unsqueeze(2).unsqueeze(3).unsqueeze(4).expand_as(x)\n",
        "\n",
        "        return x * scale\n",
        "\n",
        "\n",
        "class ChannelGate_layer3(nn.Module):\n",
        "    def __init__(self, pool_types=['avg', 'max']):\n",
        "        super(ChannelGate_layer3, self).__init__()\n",
        "        self.gate_channels = 4\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.gate_channels, self.gate_channels // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.gate_channels // 2, self.gate_channels)\n",
        "        )\n",
        "        self.pool_types = pool_types\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute((0, 2, 1, 3, 4))\n",
        "        channel_att_sum = True\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type == 'avg':\n",
        "                avg_pool = nn.AvgPool3d((x.size(2), x.size(3), x.size(4)), stride=(x.size(2), x.size(3), x.size(4)))\n",
        "                x_mid = avg_pool(x)\n",
        "                channel_att_raw = self.mlp(x_mid)\n",
        "            elif pool_type == 'max':\n",
        "                max_pool = nn.MaxPool3d((x.size(2), x.size(3), x.size(4)), stride=(x.size(2), x.size(3), x.size(4)))\n",
        "                x_mid = max_pool(x)\n",
        "                channel_att_raw = self.mlp(x_mid)\n",
        "\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = torch.sigmoid(channel_att_raw).unsqueeze(2).unsqueeze(3).unsqueeze(4).expand_as(x)\n",
        "\n",
        "        return x * scale\n",
        "\n",
        "\n",
        "class ChannelGate_layer4(nn.Module):\n",
        "    def __init__(self, pool_types=['avg', 'max']):\n",
        "        super(ChannelGate_layer4, self).__init__()\n",
        "        self.gate_channels = 2\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.gate_channels, self.gate_channels),\n",
        "        )\n",
        "        self.pool_types = pool_types\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute((0, 2, 1, 3, 4))\n",
        "        channel_att_sum = True\n",
        "        for pool_type in self.pool_types:\n",
        "            if pool_type == 'avg':\n",
        "                avg_pool = nn.AvgPool3d((x.size(2), x.size(3), x.size(4)), stride=(x.size(2), x.size(3), x.size(4)))\n",
        "                x_mid = avg_pool(x)\n",
        "                channel_att_raw = self.mlp(x_mid)\n",
        "            elif pool_type == 'max':\n",
        "                max_pool = nn.MaxPool3d((x.size(2), x.size(3), x.size(4)), stride=(x.size(2), x.size(3), x.size(4)))\n",
        "                x_mid = max_pool(x)\n",
        "                channel_att_raw = self.mlp(x_mid)\n",
        "\n",
        "            if channel_att_sum is None:\n",
        "                channel_att_sum = channel_att_raw\n",
        "            else:\n",
        "                channel_att_sum = channel_att_sum + channel_att_raw\n",
        "\n",
        "        scale = torch.sigmoid(channel_att_raw).unsqueeze(2).unsqueeze(3).unsqueeze(4).expand_as(x)\n",
        "\n",
        "        return x * scale\n",
        "\n",
        "\n",
        "class ChannelPool(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.cat((torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1)\n",
        "\n",
        "\n",
        "class spatial_attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(spatial_attention, self).__init__()\n",
        "        self.compress = ChannelPool()\n",
        "        self.spatial_conv = nn.Conv3d(in_channels=2, out_channels=1, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm3d(1, eps=1e-5, momentum=0.01, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_compress = self.compress(x)\n",
        "        x_out = self.spatial_conv(x_compress)\n",
        "        x_out = torch.sigmoid(x_out)\n",
        "        x_out = x_out.squeeze()\n",
        "        return x_out\n",
        "\n",
        "\n",
        "class BasicBlock_2d(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(BasicBlock_2d, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock_2d only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock_2d\")\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck_2d(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        pretrained_2d = True\n",
        "    ):\n",
        "        super(Bottleneck_2d, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Conv3DSimple(nn.Conv3d):\n",
        "    def __init__(self,\n",
        "                 in_planes,\n",
        "                 out_planes,\n",
        "                 midplanes=None,\n",
        "                 stride=1,\n",
        "                 padding=1):\n",
        "\n",
        "        super(Conv3DSimple, self).__init__(\n",
        "            in_channels=in_planes,\n",
        "            out_channels=out_planes,\n",
        "            kernel_size=(3, 3, 3),\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            bias=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_downsample_stride(stride):\n",
        "        return stride, stride, stride\n",
        "        \n",
        "class Conv2Plus1D(nn.Sequential):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_planes,\n",
        "                 out_planes,\n",
        "                 midplanes,\n",
        "                 stride=1,\n",
        "                 padding=1):\n",
        "        super(Conv2Plus1D, self).__init__(\n",
        "            nn.Conv3d(in_planes, midplanes, kernel_size=(1, 3, 3),\n",
        "                      stride=(1, stride, stride), padding=(0, padding, padding),\n",
        "                      bias=False),\n",
        "            nn.BatchNorm3d(midplanes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(midplanes, out_planes, kernel_size=(3, 1, 1),\n",
        "                      stride=(stride, 1, 1), padding=(padding, 0, 0),\n",
        "                      bias=False))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_downsample_stride(stride):\n",
        "        return stride, stride, stride\n",
        "\n",
        "\n",
        "class BasicBlock_3d(nn.Module):\n",
        "\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, conv_builder, stride=1, downsample=None):\n",
        "        midplanes = (inplanes * planes * 3 * 3 * 3) // (inplanes * 3 * 3 + 3 * planes)\n",
        "\n",
        "        super(BasicBlock_3d, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            conv_builder(inplanes, planes, midplanes, stride),\n",
        "            nn.BatchNorm3d(planes),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            conv_builder(planes, planes, midplanes),\n",
        "            nn.BatchNorm3d(planes)\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "        \n",
        "        \n",
        "class Bottleneck_3d(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, conv_builder, stride=1, downsample=None):\n",
        "\n",
        "        super(Bottleneck_3d, self).__init__()\n",
        "        midplanes = (inplanes * planes * 3 * 3 * 3) // (inplanes * 3 * 3 + 3 * planes)\n",
        "\n",
        "        # 1x1x1\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv3d(inplanes, planes, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm3d(planes),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Second kernel\n",
        "        self.conv2 = nn.Sequential(\n",
        "            conv_builder(planes, planes, midplanes, stride),\n",
        "            nn.BatchNorm3d(planes),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # 1x1x1\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv3d(planes, planes * self.expansion, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm3d(planes * self.expansion)\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "        \n",
        "        \n",
        "class BasicStem(nn.Sequential):\n",
        "    \"\"\"The default conv-batchnorm-relu stem\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(BasicStem, self).__init__(\n",
        "            nn.Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2),\n",
        "                      padding=(1, 3, 3), bias=False),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "\n",
        "class R2Plus1dStem(nn.Sequential):\n",
        "    \"\"\"R(2+1)D stem is different than the default one as it uses separated 3D convolution\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(R2Plus1dStem, self).__init__(\n",
        "            nn.Conv3d(3, 45, kernel_size=(1, 7, 7),\n",
        "                      stride=(1, 2, 2), padding=(0, 3, 3),\n",
        "                      bias=False),\n",
        "            nn.BatchNorm3d(45),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(45, 64, kernel_size=(3, 1, 1),\n",
        "                      stride=(1, 1, 1), padding=(1, 0, 0),\n",
        "                      bias=False),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "\n",
        "class res_3_2d_net(nn.Module):\n",
        "    \"\"\"\n",
        "    The res_3_2d_net combines resnet3d and resnet2d for model fusion.\n",
        "    \"\"\"\n",
        "    # r3d\n",
        "    # def __init__(self, arch_3d='r3d_18', arch_2d='resnet18', pretrained_3d=True, pretrained_2d=True, progress= True,\n",
        "    #              block_3d=BasicBlock_3d, block_2d=BasicBlock_2d, conv_makers=[Conv3DSimple] * 4,\n",
        "    #              layers=[2, 2, 2, 2], stem=BasicStem, num_classes=400,\n",
        "    #              zero_init_residual_3d=False, zero_init_residual_2d=False,\n",
        "    #              groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):\n",
        "\n",
        "    # r2plus1d_18\n",
        "    def __init__(self, arch_3d='r2plus1d_18', arch_2d='resnet18', pretrained_3d=True, pretrained_2d=True, progress=True,\n",
        "                 block_3d=BasicBlock_3d, block_2d=BasicBlock_2d, conv_makers=[Conv2Plus1D] * 4,\n",
        "                 layers=[2, 2, 2, 2], stem=R2Plus1dStem, num_classes=400,\n",
        "                 zero_init_residual_3d=False, zero_init_residual_2d=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):\n",
        "                 \n",
        "        super(res_3_2d_net, self).__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer_2d = norm_layer\n",
        "\n",
        "        self.inplanes_2d = 64\n",
        "        self.dilation_2d = 1\n",
        "        self.inplanes_3d = 64\n",
        "\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "\n",
        "        self.groups_2d = groups\n",
        "        self.base_width_2d = width_per_group\n",
        "        self.conv1_2d = nn.Conv2d(3, self.inplanes_2d, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1_2d = norm_layer(self.inplanes_2d)\n",
        "        self.relu_2d = nn.ReLU(inplace=True)\n",
        "        self.maxpool_2d = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1_2d = self._make_layer_2d(block_2d, 64, layers[0])\n",
        "        self.layer2_2d = self._make_layer_2d(block_2d, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3_2d = self._make_layer_2d(block_2d, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4_2d = self._make_layer_2d(block_2d, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool_2d = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.stem_3d = stem()\n",
        "        self.layer1_3d = self._make_layer_3d(block_3d, conv_makers[0], 64, layers[0], stride=1)\n",
        "        self.layer2_3d = self._make_layer_3d(block_3d, conv_makers[1], 128, layers[1], stride=2)\n",
        "        self.layer3_3d = self._make_layer_3d(block_3d, conv_makers[2], 256, layers[2], stride=2)\n",
        "        self.layer4_3d = self._make_layer_3d(block_3d, conv_makers[3], 512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool_3d = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "\n",
        "        self.spatial_pathway1 = spatial_attention()\n",
        "        self.spatial_pathway2 = spatial_attention()\n",
        "        self.spatial_pathway3 = spatial_attention()\n",
        "        self.spatial_pathway4 = spatial_attention()\n",
        "\n",
        "        self.pool_types = ['avg', 'max']\n",
        "        self.ChannelGate1 = ChannelGate_layer1(self.pool_types)\n",
        "        self.ChannelGate2 = ChannelGate_layer2(self.pool_types)\n",
        "        self.ChannelGate3 = ChannelGate_layer3(self.pool_types)\n",
        "        self.ChannelGate4 = ChannelGate_layer4(self.pool_types)\n",
        "\n",
        "        # init weights\n",
        "        self._initialize_weights_3d()\n",
        "\n",
        "        if zero_init_residual_3d:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck_3d):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual_2d:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck_2d):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock_2d):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "        if pretrained_3d:\n",
        "            state_dict_3d = load_state_dict_from_url(model_urls[arch_3d], progress=progress)\n",
        "            keys_org = [*state_dict_3d.keys()]\n",
        "            state_dict_chg_3d = state_dict_3d\n",
        "\n",
        "            # Generate new key names which equals to the model's key\n",
        "            new_key = []\n",
        "            for keys in state_dict_chg_3d:\n",
        "                split_key = keys.split('.')\n",
        "                split_key[0] = split_key[0] + '_3d'\n",
        "                join_key = '.'.join(split_key)\n",
        "                new_key.append(join_key)\n",
        "\n",
        "            # update key names for the pretrained model\n",
        "            for ind in range(len(keys_org)):\n",
        "                state_dict_chg_3d[new_key[ind]] = state_dict_chg_3d.pop(keys_org[ind])\n",
        "\n",
        "            s_dict = self.state_dict()\n",
        "            pretrained_dict_3d = {k: v for k, v in state_dict_chg_3d.items() if k in s_dict}\n",
        "            s_dict.update(pretrained_dict_3d)\n",
        "            self.load_state_dict(s_dict)\n",
        "\n",
        "        if pretrained_2d:\n",
        "            state_dict_2d = load_state_dict_from_url(model_urls[arch_2d], progress=progress)\n",
        "            keys_org_2d = list(state_dict_2d.keys())\n",
        "            state_dict_chg_2d = state_dict_2d\n",
        "            # Generate new key names which equals to the model's key\n",
        "            new_key_2d = []\n",
        "            for keys in state_dict_chg_2d:\n",
        "                split_key = keys.split('.')\n",
        "                split_key[0] = split_key[0] + '_2d'\n",
        "                join_key = '.'.join(split_key)\n",
        "                new_key_2d.append(join_key)\n",
        "\n",
        "            # update key names for the pretrained model\n",
        "            for ind in range(len(keys_org_2d)):\n",
        "                state_dict_chg_2d[new_key_2d[ind]] = state_dict_chg_2d.pop(keys_org_2d[ind])\n",
        "\n",
        "            s_dict = self.state_dict()\n",
        "            pretrained_dict_2d = {k: v for k, v in state_dict_chg_2d.items() if k in s_dict}\n",
        "            s_dict.update(pretrained_dict_2d)\n",
        "            self.load_state_dict(s_dict)\n",
        "\n",
        "    def forward(self, x_vid, x_im):\n",
        "    \n",
        "        x_vid = self.stem_3d(x_vid)\n",
        "\n",
        "        x_im = self.conv1_2d(x_im)\n",
        "        x_im = self.bn1_2d(x_im)\n",
        "        x_im = self.relu_2d(x_im)\n",
        "        x_im = self.maxpool_2d(x_im)\n",
        "\n",
        "        # Layer 1 Fusion\n",
        "        x_vid = self.layer1_3d(x_vid)\n",
        "        x_im = self.layer1_2d(x_im)\n",
        "        x_att = self.ChannelGate1(x_vid)\n",
        "        x_att = self.spatial_pathway1(x_att)\n",
        "        x_im = x_im*x_att\n",
        "\n",
        "        # Layer 2 Fusion\n",
        "        x_vid = self.layer2_3d(x_vid)\n",
        "        x_im = self.layer2_2d(x_im)\n",
        "        x_att = self.ChannelGate2(x_vid)\n",
        "        x_att = self.spatial_pathway2(x_att)\n",
        "        x_im = x_im*x_att\n",
        "\n",
        "        # Layer 3 Fusion\n",
        "        x_vid = self.layer3_3d(x_vid)\n",
        "        x_im = self.layer3_2d(x_im)\n",
        "        x_att = self.ChannelGate3(x_vid)\n",
        "        x_att = self.spatial_pathway3(x_att)\n",
        "        x_im = x_im*x_att\n",
        "\n",
        "        # Layer 4 Fusion\n",
        "        x_vid = self.layer4_3d(x_vid)\n",
        "        x_im = self.layer4_2d(x_im)\n",
        "        x_att = self.ChannelGate4(x_vid)\n",
        "        x_att = self.spatial_pathway4(x_att)\n",
        "        x_im = x_im*x_att\n",
        "\n",
        "        #\n",
        "        x_vid = self.avgpool_3d(x_vid)\n",
        "        x_im = self.avgpool_2d(x_im)\n",
        "\n",
        "        x_vid = x_vid.flatten(1)\n",
        "        x_im = x_im.flatten(1)\n",
        "\n",
        "        x_vid = x_vid.view(x_vid.size(0), -1)\n",
        "        x_im = x_im.view(x_im.size(0), -1)\n",
        "\n",
        "        return x_vid, x_im\n",
        "\n",
        "    def _make_layer_2d(self, block: Type[Union[BasicBlock_2d, Bottleneck_2d]], planes: int, blocks: int,\n",
        "                    stride: int = 1, dilate: bool = False):\n",
        "        norm_layer = self._norm_layer_2d\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation_2d\n",
        "        if dilate:\n",
        "            self.dilation_2d *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes_2d != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes_2d, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes_2d, planes, stride, downsample, self.groups_2d,\n",
        "                            self.base_width_2d, previous_dilation, norm_layer))\n",
        "        self.inplanes_2d = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes_2d, planes, groups=self.groups_2d,\n",
        "                                base_width=self.base_width_2d, dilation=self.dilation_2d,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_layer_3d(self, block, conv_builder, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        if stride != 1 or self.inplanes_3d != planes * block.expansion:\n",
        "            ds_stride = conv_builder.get_downsample_stride(stride)\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv3d(self.inplanes_3d, planes * block.expansion,\n",
        "                          kernel_size=1, stride=ds_stride, bias=False),\n",
        "                nn.BatchNorm3d(planes * block.expansion)\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes_3d, planes, conv_builder, stride, downsample))\n",
        "\n",
        "        self.inplanes_3d = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes_3d, planes, conv_builder))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights_3d(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n",
        "                                        nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def get_1x_lr_params(model):\n",
        "    \"\"\"\n",
        "    This generator returns all the parameters for conv and two fc layers of the net.\n",
        "    \"\"\"\n",
        "    b = [model.conv1_2d, model.bn1_2d, model.layer1_2d, model.layer2_2d, model.layer3_2d, model.layer4_2d,\n",
        "         model.avgpool_2d, model.stem_3d, model.layer1_3d, model.layer2_3d, model.layer3_3d, model.layer4_3d, model.avgpool_3d]\n",
        "    for i in range(len(b)):\n",
        "        for k in b[i].parameters():\n",
        "            if k.requires_grad:\n",
        "                yield k\n",
        "\n",
        "def get_10x_lr_params(model):\n",
        "    \"\"\"\n",
        "    This generator returns all the parameters for the last fc layer of the net.\n",
        "    \"\"\"\n",
        "    b = [model.layer1_c1, model.layer1_c2, model.layer1_c3, model.layer2_c1, model.layer2_c2, model.layer2_c3,\n",
        "         model.layer3_c1, model.layer3_c2, model.layer3_c3, model.layer4_c1]\n",
        "    # b = [model.lstm1]\n",
        "    for j in range(len(b)):\n",
        "        for k in b[j].parameters():\n",
        "            if k.requires_grad:\n",
        "                yield k\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pretrained_3d = True\n",
        "    pretrained_2d = True\n",
        "    progress = True\n",
        "\n",
        "    inputs1 = torch.rand(2, 3, 16, 112, 112)\n",
        "    inputs2 = torch.rand(2, 3, 224, 224)\n",
        "\n",
        "    net = res_3_2d_net()\n",
        "\n",
        "    output1, output2 = net.forward(inputs1, inputs2)\n",
        "    print('output 3d size:', output1.size(), 'output 2d size:', output2.size())\n",
        "    \n",
        "    parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "    parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
        "    print('Trainable Parameters: %.3fM' % parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-task learning wrapper"
      ],
      "metadata": {
        "id": "QWxbNNKxuPN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJP3FsmeNa0v"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generate loss wrapper for multi-task learning\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "class MultiTaskLossWrapper(nn.Module):\n",
        "    def __init__(self, task_num):\n",
        "        super(MultiTaskLossWrapper, self).__init__()\n",
        "        self.task_num = task_num\n",
        "        self.log_vars = nn.Parameter(torch.zeros(task_num))\n",
        "        print('loss log_vars', self.log_vars)\n",
        "        self.criterion1 = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, loss_mid_tot, loss_long, intent_lab, predict_intent_score, predict_activity_score, lab_seq_final):\n",
        "\n",
        "        loss0 = loss_mid_tot\n",
        "        loss1 = loss_long\n",
        "        loss2 = self.criterion1(predict_activity_score, lab_seq_final)\n",
        "        loss3 = self.criterion1(predict_intent_score, intent_lab)\n",
        "\n",
        "        precision0 = torch.exp(-self.log_vars[0])\n",
        "        loss0_p = precision0*loss0 + self.log_vars[0]\n",
        "\n",
        "        precision1 = torch.exp(-self.log_vars[1])\n",
        "        loss1_p = precision1*loss1 + self.log_vars[1]\n",
        "        #\n",
        "        precision2 = torch.exp(-self.log_vars[2])\n",
        "        loss2_p = precision2*loss2 + self.log_vars[2]\n",
        "        \n",
        "        precision3 = torch.exp(-self.log_vars[3])\n",
        "        loss3_p = precision3*loss3 + self.log_vars[3]\n",
        "        \n",
        "        print('loss_mid', loss_mid_tot)\n",
        "        print('loss_long', loss_long)\n",
        "        print('log_vars', self.log_vars)\n",
        "\n",
        "        tot_loss = loss0_p + loss1_p + loss2_p + loss3_p\n",
        "\n",
        "        return tot_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSzuLk3TomQ_"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "from datetime import datetime\n",
        "import socket\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Device being used:\", device)\n",
        "\n",
        "\n",
        "nEpochs = 80            # Number of epochs for training\n",
        "resume_epoch = 0        # Default is 0, change if want to resume\n",
        "useTest = True          # See evolution of the test set when training\n",
        "nTestInterval = 1       # Run on test set every nTestInterval epochs\n",
        "snapshot = 100       # Store a model every snapshot epochs\n",
        "lr = 1e-3               # Learning rate\n",
        "# pre_h = 1\n",
        "\n",
        "stop_time_init = str(timeit.default_timer())\n",
        "\n",
        "dataset = 'face'  # Options: hmdb51 or ucf101\n",
        "\n",
        "if dataset == 'face':\n",
        "    num_classes = 5             # long-term activity\n",
        "    num_activtites = 4          # mid-term activity\n",
        "elif dataset == 'gtea':\n",
        "    num_classes = 7\n",
        "    num_activtites = 79\n",
        "elif dataset == 'finegym':\n",
        "    num_classes = 4\n",
        "    num_activtites = 291\n",
        "    \n",
        "num_tasks = 4\n",
        "require_mtl = True\n",
        "\n",
        "save_dir_root = '/content/drive/MyDrive/carknows'\n",
        "print('save_dir_root is:', save_dir_root)\n",
        "\n",
        "if resume_epoch != 0:\n",
        "    runs = sorted(glob.glob(os.path.join(save_dir_root, 'run', 'run_*')))\n",
        "    run_id = int(runs[-1].split('_')[-1]) if runs else 0\n",
        "else:\n",
        "    runs = sorted(glob.glob(os.path.join(save_dir_root, 'run', 'run_*')))\n",
        "    run_id = int(runs[-1].split('_')[-1]) + 1 if runs else 0\n",
        "\n",
        "save_dir = os.path.join(save_dir_root, 'run', 'run_' + str(run_id))\n",
        "modelName = '2classes'   \n",
        "saveName = modelName + '-' + dataset\n",
        "\n",
        "rand_flg = True\n",
        "\n",
        "print('savename {}'.format(saveName))\n",
        "\n",
        "def accuracy(output, target, topk=(1,3)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = (pred == target.unsqueeze(dim=0)).expand_as(pred)\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(1.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "def process_sig_img(clip_img_final, org_clip_img_size):\n",
        "    process_clip = torch.empty([org_clip_img_size[0],3,224,224])\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    for ind in range(org_clip_img_size[0]):\n",
        "        im_tmp = clip_img_final[ind, :, :, :]\n",
        "        im_tmp = im_tmp.permute(1, 2, 0)\n",
        "        im_tmp = im_tmp.numpy()\n",
        "        img = Image.fromarray(np.uint8(im_tmp)).convert('RGB')\n",
        "        img_tensor = preprocess(img)\n",
        "        process_clip[ind,:,:,:] = img_tensor\n",
        "\n",
        "    clip_img_final = process_clip\n",
        "\n",
        "    return clip_img_final\n",
        "\n",
        "\n",
        "def generate_last_prediction_data(clip_i, inputs, inputs_resnet, seq_labels):\n",
        "    if rand_flg:\n",
        "        rand_ind = np.random.randint(0, 15)\n",
        "    else:\n",
        "        rand_ind = 8\n",
        "\n",
        "    clip_i_final = clip_i + 16\n",
        "    clip_seg_final = inputs[:, :, clip_i:clip_i_final, :, :]\n",
        "    clip_img_final = inputs_resnet[:, clip_i + rand_ind, :, :, :]\n",
        "    org_clip_img_size = clip_img_final.size()\n",
        "\n",
        "    clip_img_final = process_sig_img(clip_img_final, org_clip_img_size)\n",
        "    lab_seq_sub_final = seq_labels[:, :, clip_i:clip_i_final]\n",
        "\n",
        "    clip_seg_final = Variable(clip_seg_final, requires_grad=False).to(device)\n",
        "    clip_img_final = Variable(clip_img_final, requires_grad=False).to(device)\n",
        "\n",
        "    lab_seq_sub_shape = lab_seq_sub_final.shape\n",
        "    seqeunce_label = np.empty([lab_seq_sub_shape[0]])\n",
        "    seqeunce_percent = np.empty([lab_seq_sub_shape[0]])\n",
        "\n",
        "    for c_ind in range(lab_seq_sub_shape[0]):\n",
        "        sub_seq = lab_seq_sub_final[c_ind, :, :]\n",
        "        sub_seq = sub_seq.squeeze()\n",
        "        sub_seq_np = sub_seq.numpy()\n",
        "        sub_count_np = Counter(sub_seq_np)\n",
        "        value, cnt = sub_count_np.most_common()[0]\n",
        "        seqeunce_label[c_ind] = value - 1\n",
        "        seqeunce_percent[c_ind] = cnt / 16    # sequence_percent not used yet \n",
        "\n",
        "    seqeunce_label = torch.from_numpy(seqeunce_label)\n",
        "    seqeunce_label = seqeunce_label.clone().detach()\n",
        "    seqeunce_label = Variable(seqeunce_label, requires_grad=False).to(device)\n",
        "    seqeunce_label = seqeunce_label.long()\n",
        "\n",
        "    seqeunce_percent = 0        # no meaning for current study\n",
        "\n",
        "    return clip_seg_final, clip_img_final, seqeunce_label, seqeunce_percent\n",
        "\n",
        "\n",
        "def wrtieresults(testing_acc_long, testing_acc_activity, testing_acc_long_pred, testing_acc_activity_pred):\n",
        "    txtfile_name = 'results.txt'\n",
        "    stop_time = str(timeit.default_timer())\n",
        "    txtfile_name = stop_time + txtfile_name\n",
        "\n",
        "    content = np.vstack((testing_acc_long, testing_acc_activity, testing_acc_long_pred, testing_acc_activity_pred))\n",
        "\n",
        "    if os.path.exists(txtfile_name):\n",
        "        os.remove(txtfile_name)\n",
        "\n",
        "    with open(txtfile_name, \"a+\") as f:\n",
        "        for i in range(4):\n",
        "            f.writelines(str(content[i, :]))\n",
        "            f.writelines(\"\\n\")\n",
        "\n",
        "def wrtieresults_train(testing_acc_long, testing_acc_activity, testing_acc_long_pred, testing_acc_activity_pred):\n",
        "    txtfile_name = 'results_train.txt'\n",
        "    stop_time = str(timeit.default_timer())\n",
        "    txtfile_name = stop_time + txtfile_name\n",
        "\n",
        "    content = np.vstack((testing_acc_long, testing_acc_activity, testing_acc_long_pred, testing_acc_activity_pred))\n",
        "\n",
        "    if os.path.exists(txtfile_name):\n",
        "        os.remove(txtfile_name)\n",
        "\n",
        "    with open(txtfile_name, \"a+\") as f:\n",
        "        for i in range(4):\n",
        "            f.writelines(str(content[i, :]))\n",
        "            f.writelines(\"\\n\")\n",
        "\n",
        "\n",
        "def wrtieresults_top_pred(testing_acc_long, testing_acc_activity, testing_acc_long_pred, testing_acc_activity_pred):\n",
        "    txtfile_name = 'results_top_prediction.txt'\n",
        "    stop_time = str(timeit.default_timer())\n",
        "    txtfile_name = stop_time + txtfile_name\n",
        "\n",
        "    content = np.vstack((testing_acc_long, testing_acc_activity, testing_acc_long_pred, testing_acc_activity_pred))\n",
        "\n",
        "    print(content)\n",
        "    if os.path.exists(txtfile_name):\n",
        "        os.remove(txtfile_name)\n",
        "\n",
        "    with open(txtfile_name, \"a+\") as f:\n",
        "        for i in range(4):\n",
        "            f.writelines(str(content[i, :]))\n",
        "            f.writelines(\"\\n\")\n",
        "\n",
        "\n",
        "def wrtieresults_top_recog(testing_acc_long, testing_acc_activity, testing_acc_long_pred, testing_acc_activity_pred):\n",
        "    txtfile_name = 'results_top_recogniton.txt'\n",
        "    stop_time = str(timeit.default_timer())\n",
        "    txtfile_name = stop_time + txtfile_name\n",
        "\n",
        "    content = np.vstack((testing_acc_long, testing_acc_activity, testing_acc_long_pred, testing_acc_activity_pred))\n",
        "\n",
        "    print(content)\n",
        "    if os.path.exists(txtfile_name):\n",
        "        os.remove(txtfile_name)\n",
        "\n",
        "    with open(txtfile_name, \"a+\") as f:\n",
        "        for i in range(4):\n",
        "            f.writelines(str(content[i, :]))\n",
        "            f.writelines(\"\\n\")\n",
        "\n",
        "\n",
        "def wrtie_confusion_results(training_intent_confuse, training_intent_lab_confuse, training_activity_confuse, training_activity_lab_confuse, txtfile_name):\n",
        "    stop_time = stop_time_init\n",
        "    txtfile_name1 = stop_time + '_int' + txtfile_name\n",
        "    txtfile_name2 = stop_time + '_act' + txtfile_name\n",
        "\n",
        "    content1 = np.vstack((training_intent_confuse, training_intent_lab_confuse))\n",
        "    content2 = np.vstack((training_activity_confuse, training_activity_lab_confuse))\n",
        "\n",
        "    np.savetxt(txtfile_name1, np.round(content1), delimiter=',')\n",
        "    np.savetxt(txtfile_name2, np.round(content2), delimiter = ',')\n",
        "\n",
        "\n",
        "def train_model(dataset=dataset, save_dir=save_dir, num_classes=num_classes, num_activities=num_activtites,  lr=lr,\n",
        "                num_epochs=nEpochs, save_epoch=snapshot, useTest=useTest, test_interval=nTestInterval, pred_horizon=1):\n",
        "    # pred_horizon = 3\n",
        "    print('current training pred_horizon', pred_horizon)\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            num_classes (int): Number of classes in the data\n",
        "            num_epochs (int, optional): Number of epochs to train for.\n",
        "    \"\"\"\n",
        "    fusion_dim = 9 - pred_horizon\n",
        "\n",
        "    if modelName == '2classes':\n",
        "        encoder_model = res_3_2d_net()\n",
        "        fusion_model = Fusion_net_2(num_classes=num_activities,pred_horizon=fusion_dim)\n",
        "        decoder_model = LSTM_ANNO(num_classes=num_classes)\n",
        "        predict_model = prediction_net(num_activity_classes=num_activities, num_intent_classes=num_classes, fusion_ind=True, pred_horizon=fusion_dim)\n",
        "        \n",
        "        if require_mtl:\n",
        "            mt_loss = MultiTaskLossWrapper(task_num=num_tasks)\n",
        "            print('loss parameters',[*mt_loss.parameters()])\n",
        "        \n",
        "    else:\n",
        "        print('We only implemented C3D and R2Plus1D models.')\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if resume_epoch == 0:\n",
        "        print(\"Training {} from scratch...\".format(modelName))\n",
        "    else:\n",
        "        checkpoint = torch.load(\n",
        "            os.path.join(save_dir, 'models', saveName + '_epoch-' + str(resume_epoch) + '.pth'),\n",
        "            map_location=lambda storage, loc: storage)  # Load all tensors onto the CPU\n",
        "        \n",
        "        print(\"Initializing weights from: {}...\".format(\n",
        "            os.path.join(save_dir, 'models', saveName + '_epoch-' + str(resume_epoch) + '.pth')))\n",
        "        encoder_model.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "        decoder_model.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "        predict_model.load_state_dict(checkpoint['predict_state_dict'])\n",
        "        fusion_model.load_state_dict(checkpoint['fusion_state_dict'])\n",
        "        mt_loss.load_state_dict(checkpoint['mt_loss'])\n",
        "\n",
        "    print('Total params: %.2fM' % (sum(p.numel() for p in encoder_model.parameters()) / 1000000.0))\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()  # standard crossentropy loss for classification\n",
        "    criterion1 = nn.MSELoss()\n",
        "\n",
        "    encoder_model.to(device)\n",
        "    fusion_model.to(device)\n",
        "    mt_loss.to(device)\n",
        "    decoder_model.to(device)\n",
        "    predict_model.to(device)\n",
        "    \n",
        "    train_params = [\n",
        "                    {'params': encoder_model.parameters(), 'lr': lr / 10},\n",
        "                    {'params': mt_loss.parameters(), 'lr': lr},\n",
        "                    {'params': Fusion_net_2.get_1x_lr_params(fusion_model), 'lr': lr / 10},\n",
        "                    {'params': fusion_model.spat_weigh, 'lr': lr },\n",
        "                    {'params': fusion_model.temp_weigh, 'lr': lr },\n",
        "                    {'params': prediction_net.get_1x_lr_params(predict_model), 'lr': lr / 10},\n",
        "                    {'params': predict_model.intent_weigh, 'lr': lr },\n",
        "                    {'params': predict_model.act_weigh, 'lr': lr },\n",
        "                    {'params': decoder_model.parameters(), 'lr': lr / 10}]\n",
        "    print('train_params', train_params)\n",
        "\n",
        "    optimizer = optim.Adam(train_params, lr=lr, betas=(0.9, 0.999), weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=120,\n",
        "                                          gamma=0.1)  # the scheduler divides the lr by 10 every 10 epochs\n",
        "    if resume_epoch != 0:\n",
        "      optimizer.load_state_dict(checkpoint['opt_dict'])\n",
        "      for state in optimizer.state.values():\n",
        "          for k, v in state.items():\n",
        "              if torch.is_tensor(v):\n",
        "                  state[k] = v.cuda()                                      \n",
        "\n",
        "    log_dir = os.path.join(save_dir, 'models', datetime.now().strftime('%b%d_%H-%M-%S') + '_' + socket.gethostname())\n",
        "    print('tensorboard log_dir', log_dir)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "    print('Training model on {} dataset...'.format(dataset))\n",
        "\n",
        "    bat_size = 3\n",
        "    clp_len = 150\n",
        "\n",
        "    mean = [0.43216, 0.394666, 0.37645]\n",
        "    std = [0.22803, 0.22145, 0.216989]\n",
        "    resize_size = 224\n",
        "    crop_size = 112\n",
        "\n",
        "    mean_res = [0.485, 0.456, 0.406]\n",
        "    std_res = [0.229, 0.224, 0.225]\n",
        "\n",
        "    trans1 = [\n",
        "        ConvertBHWCtoBCHW(),\n",
        "        transforms.ConvertImageDtype(torch.float32),\n",
        "    ]\n",
        "    trans1.extend([\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "        ConvertBCHWtoCBHW()])\n",
        "\n",
        "    trans2 = [\n",
        "        ConvertBHWCtoBCHW(),\n",
        "        transforms.ConvertImageDtype(torch.float32),\n",
        "    ]\n",
        "    trans2.extend([\n",
        "        transforms.Normalize(mean=mean_res, std=std_res)])\n",
        "\n",
        "    transform1_t = transforms.Compose(trans1)\n",
        "    transform2_t = transforms.Compose(trans2)\n",
        "\n",
        "    train_dataloader = DataLoader(CarDataset_multi_2(dataset=dataset, split='train', clip_len=clp_len,\n",
        "                                                  transform1=transform1_t, transform2=None),\n",
        "                                  batch_size=bat_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    val_dataloader = DataLoader(CarDataset_multi_2(dataset=dataset, split='val', clip_len=clp_len,\n",
        "                                                transform1=transform1_t, transform2=None),\n",
        "                                batch_size=bat_size, num_workers=4, pin_memory=True)\n",
        "\n",
        "    test_dataloader = DataLoader(CarDataset_multi_2(dataset=dataset, split='test', clip_len=clp_len,\n",
        "                                                 transform1=transform1_t, transform2=None),\n",
        "                                 batch_size=bat_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    trainval_loaders = {'train': train_dataloader, 'val': val_dataloader}\n",
        "    trainval_sizes = {x: len(trainval_loaders[x].dataset) for x in ['train', 'val']}\n",
        "    test_size = len(test_dataloader.dataset)\n",
        "\n",
        "    training_loss = []\n",
        "    training_acc_long = []\n",
        "    training_acc_activity = []\n",
        "    training_acc_long_pred = []\n",
        "    training_acc_activity_pred = []\n",
        "\n",
        "    training_acc_top1 = []\n",
        "    training_acc_top3 = []\n",
        "    training_acc_top1_MR = []\n",
        "    training_acc_top3_MR = []\n",
        "    training_acc_top1_IP = []\n",
        "    training_acc_top3_IP = []\n",
        "    training_acc_top1_MP = []\n",
        "    training_acc_top3_MP = []\n",
        "\n",
        "    testing_loss = []\n",
        "    testing_acc_long = []\n",
        "    testing_acc_activity = []\n",
        "    testing_acc_long_pred = []\n",
        "    testing_acc_activity_pred = []\n",
        "    testing_acc_top1 = []\n",
        "    testing_acc_top3 = []\n",
        "    testing_acc_top1_MR = []\n",
        "    testing_acc_top3_MR = []\n",
        "    testing_acc_top1_IP = []\n",
        "    testing_acc_top3_IP = []\n",
        "    testing_acc_top1_MP = []\n",
        "    testing_acc_top3_MP = []\n",
        "\n",
        "    max_acc = 0\n",
        "\n",
        "    for epoch in range(resume_epoch, num_epochs):\n",
        "        # each epoch has a training and validation step\n",
        "        print('epoch {}'.format(epoch))\n",
        "        for phase in ['train']:\n",
        "            start_time = timeit.default_timer()\n",
        "            print('start time', start_time)\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects_long = 0.0\n",
        "            running_corrects_mid = 0.0\n",
        "            running_corrects_long_pred = 0.0\n",
        "            running_corrects_mid_pred = 0.0\n",
        "\n",
        "            running_corrects_top1 = 0.0\n",
        "            running_corrects_top3 = 0.0\n",
        "\n",
        "            running_corrects_top1_MR = 0.0\n",
        "            running_corrects_top3_MR = 0.0\n",
        "\n",
        "            running_corrects_top1_IP = 0.0\n",
        "            running_corrects_top3_IP = 0.0\n",
        "\n",
        "            running_corrects_top1_MP = 0.0\n",
        "            running_corrects_top3_MP = 0.0\n",
        "\n",
        "            # reset the running loss and corrects\n",
        "            # set model to train() or eval() mode depending on whether it is trained\n",
        "            # or being validated. Primarily affects layers such as BatchNorm or Dropout.\n",
        "            if phase == 'train':\n",
        "                # scheduler.step() is to be called once every epoch during training\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                encoder_model.train()\n",
        "                decoder_model.train()\n",
        "                predict_model.train()\n",
        "                mt_loss.train()\n",
        "                fusion_model.train()\n",
        "\n",
        "            else:\n",
        "                encoder_model.eval()\n",
        "                decoder_model.eval()\n",
        "                predict_model.eval()\n",
        "                mt_loss.eval()\n",
        "                fusion_model.eval()\n",
        "\n",
        "\n",
        "            for inputs, inputs_resnet, labels, seq_labels in tqdm(trainval_loaders[phase]):\n",
        "                # move inputs and labels to the device the training is taking place on\n",
        "                labels = labels.clone().detach()\n",
        "                labels = labels.long()\n",
        "                labels = Variable(labels, requires_grad=False).to(device)\n",
        "                print('labels', labels.size(), labels)\n",
        "\n",
        "                batch_size, C, frames, H, W = inputs.shape\n",
        "\n",
        "                clip_feats = torch.Tensor([]).to(device)\n",
        "                clip_activity_feats = torch.Tensor([]).to(device)\n",
        "                tot_activity_pred = []\n",
        "                tot_seq_lab = []\n",
        "                clip_ind = 0\n",
        "                init_flag = True\n",
        "                pre1_sum = torch.Tensor([]).to(device)\n",
        "                pre3_sum = torch.Tensor([]).to(device)\n",
        "\n",
        "                loss_mid_tot = 0\n",
        "                loss_cos = 0\n",
        "                loss_mid_reg = 0\n",
        "                loss_mid = 0\n",
        "                ind = 0\n",
        "\n",
        "                clip_len = len(np.arange(0, frames-(17+pred_horizon*16), 16))\n",
        "                pred_ind = frames-17\n",
        "                for clip_i in np.arange(0, frames-(17+pred_horizon*16), 16):\n",
        "                    ind += 1\n",
        "                    clip_seg, clip_img, seqeunce_label, seqeunce_percent = generate_last_prediction_data(clip_i, inputs, inputs_resnet, seq_labels)\n",
        "\n",
        "                    clip_feats_temp, sig_feats_temp = encoder_model(clip_seg, clip_img)\n",
        "                    clip_feats_int, activity_pred, activity_output, [spat_weigh, temp_weigh] = fusion_model(clip_feats_temp, sig_feats_temp)\n",
        "\n",
        "                    loss_mid = criterion(activity_pred, seqeunce_label)\n",
        "                    loss_mid_tot += loss_mid\n",
        "\n",
        "                    prec1, prec3 = accuracy(activity_pred.data, seqeunce_label.data, topk=(1, 3))\n",
        "\n",
        "                    pre1_sum = torch.cat([pre1_sum, prec1])\n",
        "                    pre3_sum = torch.cat([pre3_sum, prec3])\n",
        "\n",
        "                    if init_flag:\n",
        "                        probs_mid_sub = nn.Softmax(dim=1)(activity_pred)\n",
        "                        preds_mid_sub = torch.max(probs_mid_sub, 1)[1]\n",
        "                        seq_labs_mid = seqeunce_label\n",
        "                        init_flag = False\n",
        "                    else:\n",
        "                        probs_mid_sub = nn.Softmax(dim=1)(activity_pred)\n",
        "                        preds_mid_sub_s = torch.max(probs_mid_sub, 1)[1]\n",
        "                        preds_mid_sub = torch.cat((preds_mid_sub, preds_mid_sub_s))\n",
        "                        seq_labs_mid = torch.cat((seq_labs_mid, seqeunce_label))\n",
        "\n",
        "                    clip_feats = torch.cat((clip_feats, clip_feats_int), 1)\n",
        "                    clip_activity_feats = torch.cat((clip_activity_feats, activity_output), 1)\n",
        "\n",
        "                print('loss_mid_tot', loss_mid_tot, loss_mid_tot.size())\n",
        "\n",
        "                prec1_MR = torch.mean(pre1_sum)\n",
        "                prec3_MR = torch.mean(pre3_sum)\n",
        "\n",
        "                clip_seg_final, clip_img_final, lab_seq_final, lab_percent_final = generate_last_prediction_data(pred_ind, inputs, inputs_resnet, seq_labels)\n",
        " \n",
        "                pred_final_score, gru_output = decoder_model(clip_feats)\n",
        "                predict_intent_score, predict_activity_score, [int_weigh, act_weigh] = predict_model(clip_activity_feats, gru_output)\n",
        "\n",
        "                print('fusion module spaial:', spat_weigh, 'fusion module temporal:', temp_weigh)\n",
        "                print('predict module intent:', int_weigh, 'predict module activity', act_weigh)\n",
        "\n",
        "                probs_pred_long = nn.Softmax(dim=1)(predict_intent_score)\n",
        "                preds_pred_long = torch.max(probs_pred_long, 1)[1]\n",
        "\n",
        "                probs_pred_mid = nn.Softmax(dim=1)(predict_activity_score)\n",
        "                preds_pred_mid = torch.max(probs_pred_mid, 1)[1]\n",
        "\n",
        "                probs = nn.Softmax(dim=1)(pred_final_score)\n",
        "                preds = torch.max(probs, 1)[1]\n",
        "\n",
        "                preds_mid = preds_mid_sub\n",
        "                seq_labs = seq_labs_mid\n",
        "\n",
        "                prec1_LI, prec3_LI = accuracy(pred_final_score.data, labels.data, topk=(1, 3))\n",
        "                prec1_LP, prec3_LP = accuracy(predict_intent_score.data, labels.data, topk=(1, 3))\n",
        "                prec1_MP, prec3_MP = accuracy(predict_activity_score.data, lab_seq_final.data, topk=(1, 3))\n",
        "\n",
        "                if require_mtl:\n",
        "                    loss_long = criterion(pred_final_score, labels)\n",
        "                    loss_tot = mt_loss(loss_mid_tot, loss_long, labels, predict_intent_score,\n",
        "                                       predict_activity_score, lab_seq_final)\n",
        "                    print('loss_tot', loss_tot, loss_tot.size())\n",
        "                else:\n",
        "                    loss_mid = criterion(activity_pred, seqeunce_label)\n",
        "                    print('loss_mid', loss_mid)\n",
        "                    loss_long = criterion(pred_final_score, labels)\n",
        "                    print('loss_long', loss_long)\n",
        "                    loss_tot = loss_long + (loss_mid)\n",
        "                    print('loss_tot', loss_tot)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss_tot.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                running_loss += loss_tot.item() * inputs.size(0)\n",
        "                running_corrects_long += torch.sum(preds == labels.data)\n",
        "                running_corrects_mid += torch.sum(preds_mid == seq_labs.data)\n",
        "\n",
        "                running_corrects_long_pred += torch.sum(preds_pred_long == labels.data)\n",
        "                running_corrects_mid_pred += torch.sum(preds_pred_mid == lab_seq_final.data)\n",
        "\n",
        "                running_corrects_top1 += torch.sum(prec1_LI)\n",
        "                running_corrects_top3 += torch.sum(prec3_LI)\n",
        "\n",
        "                running_corrects_top1_MR += torch.sum(prec1_MR)\n",
        "                running_corrects_top3_MR += torch.sum(prec3_MR)\n",
        "\n",
        "                running_corrects_top1_IP += torch.sum(prec1_LP)\n",
        "                running_corrects_top3_IP += torch.sum(prec3_LP)\n",
        "\n",
        "                running_corrects_top1_MP += torch.sum(prec1_MP)\n",
        "                running_corrects_top3_MP += torch.sum(prec3_MP)\n",
        "\n",
        "            train_epoch_loss = running_loss / trainval_sizes[phase]\n",
        "            train_epoch_acc_long = running_corrects_long.double() / trainval_sizes[phase]\n",
        "            train_epoch_acc_mid = running_corrects_mid.double() / (trainval_sizes[phase]*(clip_len))\n",
        "            train_epoch_acc_long_pred = running_corrects_long_pred.double() / trainval_sizes[phase]\n",
        "            train_epoch_acc_mid_pred = running_corrects_mid_pred.double() / trainval_sizes[phase]\n",
        "\n",
        "            train_epoch_acc_top1 = running_corrects_top1.double() * bat_size / trainval_sizes[phase]\n",
        "            train_epoch_acc_top3 = running_corrects_top3.double() * bat_size / trainval_sizes[phase]\n",
        "\n",
        "            train_epoch_acc_top1_MR = running_corrects_top1_MR.double() * bat_size / trainval_sizes[phase]\n",
        "            train_epoch_acc_top3_MR = running_corrects_top3_MR.double() * bat_size / trainval_sizes[phase]\n",
        "\n",
        "            train_epoch_acc_top1_IP = running_corrects_top1_IP.double() * bat_size / trainval_sizes[phase]\n",
        "            train_epoch_acc_top3_IP = running_corrects_top3_IP.double() * bat_size / trainval_sizes[phase]\n",
        "\n",
        "            train_epoch_acc_top1_MP = running_corrects_top1_MP.double() * bat_size / trainval_sizes[phase]\n",
        "            train_epoch_acc_top3_MP = running_corrects_top3_MP.double() * bat_size / trainval_sizes[phase]\n",
        "\n",
        "            print('train_epoch_acc_top1', train_epoch_acc_top1,' train_epoch_acc_top3', train_epoch_acc_top3)\n",
        "\n",
        "            train_epoch_acc_cpu_long = train_epoch_acc_long.data.cpu().numpy()\n",
        "            train_epoch_acc_cpu_mid = train_epoch_acc_mid.data.cpu().numpy()\n",
        "            train_epoch_acc_cpu_long_pred = train_epoch_acc_long_pred.cpu().numpy()\n",
        "            train_epoch_acc_cpu_mid_pred = train_epoch_acc_mid_pred.cpu().numpy()\n",
        "\n",
        "            train_epoch_acc_cpu_top1 = train_epoch_acc_top1.cpu().numpy()\n",
        "            train_epoch_acc_cpu_top3 = train_epoch_acc_top3.cpu().numpy()\n",
        "\n",
        "            train_epoch_acc_cpu_top1_MR = train_epoch_acc_top1_MR.cpu().numpy()\n",
        "            train_epoch_acc_cpu_top3_MR = train_epoch_acc_top3_MR.cpu().numpy()\n",
        "\n",
        "            train_epoch_acc_cpu_top1_IP = train_epoch_acc_top1_IP.cpu().numpy()\n",
        "            train_epoch_acc_cpu_top3_IP = train_epoch_acc_top3_IP.cpu().numpy()\n",
        "\n",
        "            train_epoch_acc_cpu_top1_MP = train_epoch_acc_top1_MP.cpu().numpy()\n",
        "            train_epoch_acc_cpu_top3_MP = train_epoch_acc_top3_MP.cpu().numpy()\n",
        "\n",
        "            training_loss = np.append(training_loss, train_epoch_loss)\n",
        "            training_acc_long = np.append(training_acc_long, train_epoch_acc_cpu_long)\n",
        "            training_acc_activity = np.append(training_acc_activity, train_epoch_acc_cpu_mid)\n",
        "            training_acc_long_pred = np.append(training_acc_long_pred, train_epoch_acc_cpu_long_pred)\n",
        "            training_acc_activity_pred = np.append(training_acc_activity_pred, train_epoch_acc_cpu_mid_pred)\n",
        "\n",
        "            training_acc_top1 = np.append(training_acc_top1, train_epoch_acc_cpu_top1)\n",
        "            training_acc_top3 = np.append(training_acc_top3, train_epoch_acc_cpu_top3)\n",
        "            training_acc_top1_MR = np.append(training_acc_top1_MR, train_epoch_acc_cpu_top1_MR)\n",
        "            training_acc_top3_MR = np.append(training_acc_top3_MR, train_epoch_acc_cpu_top3_MR)\n",
        "\n",
        "            training_acc_top1_IP = np.append(training_acc_top1_IP, train_epoch_acc_cpu_top1_IP)\n",
        "            training_acc_top3_IP = np.append(training_acc_top3_IP, train_epoch_acc_cpu_top3_IP)\n",
        "            training_acc_top1_MP = np.append(training_acc_top1_MP, train_epoch_acc_cpu_top1_MP)\n",
        "            training_acc_top3_MP = np.append(training_acc_top3_MP, train_epoch_acc_cpu_top3_MP)\n",
        "\n",
        "            writer.add_scalar('data/train_loss_epoch', train_epoch_loss, epoch)\n",
        "            writer.add_scalar('data/train_acc_long_epoch', train_epoch_acc_long, epoch)\n",
        "            writer.add_scalar('data/train_acc_mid_epoch', train_epoch_acc_mid, epoch)\n",
        "            writer.add_scalar('data/train_acc_long_pred_epoch', train_epoch_acc_long_pred, epoch)\n",
        "            writer.add_scalar('data/train_acc_mid_pred_epoch', train_epoch_acc_mid_pred, epoch)\n",
        "\n",
        "            writer.add_scalar('data/train_top1_epoch', train_epoch_acc_cpu_top1, epoch)\n",
        "            writer.add_scalar('data/train_top3_epoch', train_epoch_acc_cpu_top3, epoch)\n",
        "            writer.add_scalar('data/train_epoch_acc_cpu_top1_MR', train_epoch_acc_cpu_top1_MR, epoch)\n",
        "            writer.add_scalar('data/train_epoch_acc_cpu_top3_MR', train_epoch_acc_cpu_top3_MR, epoch)\n",
        "            writer.add_scalar('data/train_epoch_acc_cpu_top1_IP', train_epoch_acc_cpu_top1_IP, epoch)\n",
        "            writer.add_scalar('data/train_epoch_acc_cpu_top3_IP', train_epoch_acc_cpu_top3_IP, epoch)\n",
        "            writer.add_scalar('data/train_epoch_acc_cpu_top1_MP', train_epoch_acc_cpu_top1_MP, epoch)\n",
        "            writer.add_scalar('data/train_epoch_acc_cpu_top3_MP', train_epoch_acc_cpu_top3_MP, epoch)\n",
        "\n",
        "            writer.flush()\n",
        "\n",
        "            print(\"[{}] Epoch: {}/{} Loss: {} Intent Acc: {} Activity Acc: {} Pred Intent Acc: {} Pred Activity Acc: {}\"\n",
        "                  .format(phase, epoch + 1, nEpochs, train_epoch_loss, train_epoch_acc_long, train_epoch_acc_mid,\n",
        "                          train_epoch_acc_long_pred, train_epoch_acc_mid_pred))\n",
        "\n",
        "            stop_time = timeit.default_timer()\n",
        "            print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
        "\n",
        "            if epoch % save_epoch == (save_epoch - 1):\n",
        "                torch.save({\n",
        "                    'epoch': epoch + 1,\n",
        "                    'encoder_state_dict': encoder_model.state_dict(),\n",
        "                    'decoder_state_dict': decoder_model.state_dict(),\n",
        "                    'fusion_state_dict': fusion_model.state_dict(),\n",
        "                    'predict_state_dict': predict_model.state_dict(),\n",
        "                    'mt_loss': mt_loss.state_dict(),\n",
        "                    'opt_dict': optimizer.state_dict(),\n",
        "                }, os.path.join(save_dir, 'models', saveName + '_epoch-' + str(epoch) + '.pth'))\n",
        "                print(\"Save model at {}\\n\".format(os.path.join(save_dir, 'models', saveName + '_epoch-' + str(epoch) + '.pth')))\n",
        "\n",
        "        if useTest and epoch % test_interval == (test_interval - 1):\n",
        "            encoder_model.eval()\n",
        "            mt_loss.eval()\n",
        "            fusion_model.eval()\n",
        "            decoder_model.eval()\n",
        "            predict_model.eval()\n",
        "\n",
        "            start_time = timeit.default_timer()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects_long = 0.0\n",
        "            running_corrects_mid = 0.0\n",
        "            running_corrects_long_pred = 0.0\n",
        "            running_corrects_mid_pred = 0.0\n",
        "            \n",
        "            running_corrects_top1 = 0.0\n",
        "            running_corrects_top3 = 0.0\n",
        "            running_corrects_top1_MR = 0.0\n",
        "            running_corrects_top3_MR = 0.0\n",
        "            running_corrects_top1_IP = 0.0\n",
        "            running_corrects_top3_IP = 0.0\n",
        "            running_corrects_top1_MP = 0.0\n",
        "            running_corrects_top3_MP = 0.0\n",
        "\n",
        "            running_recog_intent = []\n",
        "            running_recog_label = []\n",
        "\n",
        "            running_recog_activity = []\n",
        "            running_recog_seqlab = []\n",
        "\n",
        "            running_pred_intent = []\n",
        "            running_pred_label = []\n",
        "\n",
        "            running_pred_activity = []\n",
        "            running_pred_seqlab = []\n",
        "\n",
        "            print('---------Testing-------------------')\n",
        "            for inputs, inputs_resnet, labels, seq_labels in tqdm(test_dataloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device, dtype=torch.long)\n",
        "                with torch.no_grad():\n",
        "                    batch_size, C, frames, H, W = inputs.shape\n",
        "                    clip_feats = torch.Tensor([]).to(device)\n",
        "                    clip_activity_feats = torch.Tensor([]).to(device)\n",
        "                    \n",
        "                    pre1_sum = torch.Tensor([]).to(device)\n",
        "                    pre3_sum = torch.Tensor([]).to(device)\n",
        "                    \n",
        "                    loss_mid_tot = 0\n",
        "                    loss_mid_reg = 0\n",
        "                    loss_cos = 0\n",
        "\n",
        "                    clip_len = len(np.arange(0, frames - (17 + pred_horizon * 16), 16))\n",
        "                    pred_ind = frames - 17\n",
        "\n",
        "                    init_flag_test = True\n",
        "                    for clip_i in np.arange(0, frames - (17+pred_horizon*16), 16):\n",
        "\n",
        "                        clip_seg, clip_img, seqeunce_label, seqeunce_percent = generate_last_prediction_data(clip_i, inputs,\n",
        "                                                                                           inputs_resnet, seq_labels)\n",
        "\n",
        "                        clip_feats_temp, sig_feats_temp = encoder_model(clip_seg, clip_img)\n",
        "                       \n",
        "                        clip_feats_int, activity_pred, activity_output, [spat_weigh, temp_weigh] = fusion_model(clip_feats_temp, sig_feats_temp)\n",
        "\n",
        "                        loss_mid = criterion(activity_pred, seqeunce_label)\n",
        "                        prec1, prec3 = accuracy(activity_pred.data, seqeunce_label.data, topk=(1, 3))\n",
        "\n",
        "                        loss_mid_tot += loss_mid\n",
        "\n",
        "                        pre1_sum = torch.cat([pre1_sum, prec1])\n",
        "                        pre3_sum = torch.cat([pre3_sum, prec3])\n",
        "\n",
        "                        if init_flag_test:\n",
        "                            probs_mid_sub = nn.Softmax(dim=1)(activity_pred)\n",
        "                            preds_mid_sub = torch.max(probs_mid_sub, 1)[1]\n",
        "                            seq_labs_mid = seqeunce_label\n",
        "                            init_flag_test = False\n",
        "                        else:\n",
        "                            probs_mid_sub = nn.Softmax(dim=1)(activity_pred)\n",
        "                            preds_mid_sub_s = torch.max(probs_mid_sub, 1)[1]\n",
        "                            preds_mid_sub = torch.cat((preds_mid_sub, preds_mid_sub_s))\n",
        "                            seq_labs_mid = torch.cat((seq_labs_mid, seqeunce_label))\n",
        "\n",
        "                        clip_feats = torch.cat((clip_feats, clip_feats_int), 1)\n",
        "                        clip_activity_feats = torch.cat((clip_activity_feats, activity_output), 1)\n",
        "\n",
        "                prec1_MR = torch.mean(pre1_sum)\n",
        "                prec3_MR = torch.mean(pre3_sum)\n",
        "\n",
        "                clip_seg_final, clip_img_final, lab_seq_final, lab_percent_final = generate_last_prediction_data(pred_ind,inputs,inputs_resnet,seq_labels)\n",
        "\n",
        "                preds_mid = preds_mid_sub\n",
        "                seq_labs = seq_labs_mid\n",
        "\n",
        "                pred_final_score, gru_output = decoder_model(clip_feats)\n",
        "                predict_intent_score, predict_activity_score, [int_weigh, act_weigh] = predict_model(clip_activity_feats, gru_output)\n",
        "\n",
        "                probs_pred_long = nn.Softmax(dim=1)(predict_intent_score)\n",
        "                preds_pred_long = torch.max(probs_pred_long, 1)[1]\n",
        "\n",
        "                probs_pred_mid = nn.Softmax(dim=1)(predict_activity_score)\n",
        "                preds_pred_mid = torch.max(probs_pred_mid, 1)[1]\n",
        "\n",
        "                probs = nn.Softmax(dim=1)(pred_final_score)\n",
        "                preds = torch.max(probs, 1)[1]\n",
        "\n",
        "                prec1_LI, prec3_LI = accuracy(pred_final_score.data, labels.data, topk=(1, 3))\n",
        "                prec1_LP, prec3_LP = accuracy(predict_intent_score.data, labels.data, topk=(1, 3))\n",
        "                prec1_MP, prec3_MP = accuracy(predict_activity_score.data, lab_seq_final.data, topk=(1, 3))\n",
        "                print('prec1_MR', prec1_MR,'prec3_MR',prec3_MR,'prec1_LI',prec1_LI,'prec3_LI',prec3_LI)\n",
        "\n",
        "                # Recognize intent\n",
        "                recog_tmp_intent = preds.clone().detach().cpu().numpy()\n",
        "                recog_lab_intent = labels.clone().detach().cpu().numpy()\n",
        "                # Recognize activity\n",
        "                recog_tmp_activity = preds_mid.clone().detach().cpu().numpy()\n",
        "                lab_seq_act_recog = seq_labs.clone().detach().cpu().numpy()\n",
        "                # predict intent\n",
        "                preds_tmp_intent = preds_pred_long.clone().detach().cpu().numpy()\n",
        "                lab_tmp_intent = recog_lab_intent\n",
        "                # predict activity\n",
        "                preds_tmp_act = preds_pred_mid.clone().detach().cpu().numpy()\n",
        "                lab_seq_act_final = lab_seq_final.data.clone().detach().cpu().numpy()\n",
        "\n",
        "                if require_mtl:\n",
        "                    loss_long = criterion(pred_final_score, labels)\n",
        "                    loss_tot = mt_loss(loss_mid_tot, loss_long, labels, predict_intent_score, predict_activity_score, lab_seq_final)\n",
        "                else:\n",
        "                    loss_mid = criterion(activity_pred, seqeunce_label)\n",
        "                    print('loss_mid', loss_mid)\n",
        "                    loss_long = criterion(pred_final_score, labels)\n",
        "                    print('loss_long', loss_long)\n",
        "                    loss_tot = loss_long + (loss_mid)\n",
        "                    print('loss_tot', loss_tot)\n",
        "\n",
        "                running_loss += loss_tot.item() * inputs.size(0)\n",
        "                running_corrects_long += torch.sum(preds == labels.data)\n",
        "                running_corrects_mid += torch.sum(preds_mid == seq_labs.data)\n",
        "\n",
        "                running_corrects_long_pred += torch.sum(preds_pred_long == labels.data)\n",
        "                running_corrects_mid_pred += torch.sum(preds_pred_mid == lab_seq_final.data)\n",
        "\n",
        "                print('prec1_LI test', prec1_LI, 'prec3_LI test', prec3_LI)\n",
        "                print('running_corrects_top1',running_corrects_top1,'running_corrects_top3',running_corrects_top3)\n",
        "\n",
        "                running_corrects_top1 += torch.sum(prec1_LI)\n",
        "                running_corrects_top3 += torch.sum(prec3_LI)\n",
        "\n",
        "                running_corrects_top1_MR += torch.sum(prec1_MR)\n",
        "                running_corrects_top3_MR += torch.sum(prec3_MR)\n",
        "\n",
        "                running_corrects_top1_IP += torch.sum(prec1_LP)\n",
        "                running_corrects_top3_IP += torch.sum(prec3_LP)\n",
        "\n",
        "                running_corrects_top1_MP += torch.sum(prec1_MP)\n",
        "                running_corrects_top3_MP += torch.sum(prec3_MP)\n",
        "\n",
        "                running_recog_intent = np.append(running_recog_intent, recog_tmp_intent)\n",
        "                running_recog_label = np.append(running_recog_label, recog_lab_intent)\n",
        "\n",
        "                running_recog_activity = np.append(running_recog_activity, recog_tmp_activity)\n",
        "                running_recog_seqlab = np.append(running_recog_seqlab, lab_seq_act_recog)\n",
        "\n",
        "                running_pred_intent = np.append(running_pred_intent, preds_tmp_intent)\n",
        "                running_pred_label = np.append(running_pred_label, lab_tmp_intent)\n",
        "\n",
        "                running_pred_activity = np.append(running_pred_activity, preds_tmp_act)\n",
        "                running_pred_seqlab = np.append(running_pred_seqlab, lab_seq_act_final)\n",
        "\n",
        "            epoch_loss = running_loss / test_size\n",
        "            epoch_acc_long = running_corrects_long.double() / test_size\n",
        "            epoch_acc_mid = running_corrects_mid.double() / (test_size*(clip_len))\n",
        "            epoch_acc_long_pred = running_corrects_long_pred.double() / test_size\n",
        "            epoch_acc_mid_pred = running_corrects_mid_pred.double() / test_size\n",
        "\n",
        "            print('running_corrects_top1', running_corrects_top1, 'running_corrects_top3', running_corrects_top3)\n",
        "\n",
        "            epoch_acc_top1 = running_corrects_top1.double() * bat_size / test_size\n",
        "            epoch_acc_top3 = running_corrects_top3.double() * bat_size / test_size\n",
        "\n",
        "            epoch_acc_top1_MR = running_corrects_top1_MR.double() * bat_size / test_size\n",
        "            epoch_acc_top3_MR = running_corrects_top3_MR.double() * bat_size / test_size\n",
        "\n",
        "            epoch_acc_top1_IP = running_corrects_top1_IP.double() * bat_size / test_size\n",
        "            epoch_acc_top3_IP = running_corrects_top3_IP.double() * bat_size / test_size\n",
        "\n",
        "            epoch_acc_top1_MP = running_corrects_top1_MP.double() * bat_size / test_size\n",
        "            epoch_acc_top3_MP = running_corrects_top3_MP.double() * bat_size / test_size\n",
        "\n",
        "            print('epoch_acc_top1', epoch_acc_top1, ' epoch_acc_top3', epoch_acc_top3)\n",
        "            print('epoch_acc_top1_MR', epoch_acc_top1_MR, ' epoch_acc_top3_MR', epoch_acc_top3_MR)\n",
        "\n",
        "            epoch_acc_cpu_long = epoch_acc_long.data.cpu().numpy()\n",
        "            epoch_acc_cpu_mid = epoch_acc_mid.data.cpu().numpy()\n",
        "            epoch_acc_cpu_long_pred = epoch_acc_long_pred.data.cpu().numpy()\n",
        "            epoch_acc_cpu_mid_pred = epoch_acc_mid_pred.data.cpu().numpy()\n",
        "\n",
        "            epoch_acc_cpu_top1 = epoch_acc_top1.data.cpu().numpy()\n",
        "            epoch_acc_cpu_top3 = epoch_acc_top3.data.cpu().numpy()\n",
        "            epoch_acc_cpu_top1_MR = epoch_acc_top1_MR.data.cpu().numpy()\n",
        "            epoch_acc_cpu_top3_MR = epoch_acc_top3_MR.data.cpu().numpy()\n",
        "\n",
        "            epoch_acc_cpu_top1_IP = epoch_acc_top1_IP.data.cpu().numpy()\n",
        "            epoch_acc_cpu_top3_IP = epoch_acc_top3_IP.data.cpu().numpy()\n",
        "            epoch_acc_cpu_top1_MP = epoch_acc_top1_MP.data.cpu().numpy()\n",
        "            epoch_acc_cpu_top3_MP = epoch_acc_top3_MP.data.cpu().numpy()\n",
        "\n",
        "            testing_loss = np.append(testing_loss, epoch_loss)\n",
        "            testing_acc_long = np.append(testing_acc_long, epoch_acc_cpu_long)\n",
        "            testing_acc_activity = np.append(testing_acc_activity, epoch_acc_cpu_mid)\n",
        "            testing_acc_long_pred = np.append(testing_acc_long_pred, epoch_acc_cpu_long_pred)\n",
        "            testing_acc_activity_pred = np.append(testing_acc_activity_pred, epoch_acc_cpu_mid_pred)\n",
        "\n",
        "            testing_acc_top1 = np.append(testing_acc_top1, epoch_acc_cpu_top1)\n",
        "            testing_acc_top3 = np.append(testing_acc_top3, epoch_acc_cpu_top3)\n",
        "            testing_acc_top1_MR = np.append(testing_acc_top1_MR, epoch_acc_cpu_top1_MR)\n",
        "            testing_acc_top3_MR = np.append(testing_acc_top3_MR, epoch_acc_cpu_top3_MR)\n",
        "\n",
        "            testing_acc_top1_IP = np.append(testing_acc_top1_IP, epoch_acc_cpu_top1_IP)\n",
        "            testing_acc_top3_IP = np.append(testing_acc_top3_IP, epoch_acc_cpu_top3_IP)\n",
        "            testing_acc_top1_MP = np.append(testing_acc_top1_MP, epoch_acc_cpu_top1_MP)\n",
        "            testing_acc_top3_MP = np.append(testing_acc_top3_MP, epoch_acc_cpu_top3_MP)\n",
        "\n",
        "            print('epoch_acc_cpu_long:',epoch_acc_cpu_long, 'max_acc:', max_acc)\n",
        "            if epoch_acc_cpu_long > max_acc:\n",
        "                max_acc = epoch_acc_cpu_long\n",
        "                wrtie_confusion_results(running_recog_intent, running_recog_label, running_recog_activity,\n",
        "                                        running_recog_seqlab, '_recog_max_confusion.txt')\n",
        "                wrtie_confusion_results(running_pred_intent, running_pred_label, running_pred_activity,\n",
        "                                        running_pred_seqlab, '_pred_max_confusion.txt')\n",
        "\n",
        "                print(\"Save model at {}\\n\".format(\n",
        "                    os.path.join(save_dir, 'models', saveName + '_epoch-' + str(epoch) + '.pth')))\n",
        "\n",
        "            writer.add_scalar('data/test_max_acc_epoch', max_acc, epoch)\n",
        "            writer.add_scalar('data/test_loss_epoch', epoch_loss, epoch)\n",
        "            writer.add_scalar('data/test_acc_long_epoch', epoch_acc_long, epoch)\n",
        "            writer.add_scalar('data/test_acc_mid_epoch', epoch_acc_mid, epoch)\n",
        "            writer.add_scalar('data/test_acc_long_pred_epoch', epoch_acc_long_pred, epoch)\n",
        "            writer.add_scalar('data/test_acc_mid_pred_epoch', epoch_acc_mid_pred, epoch)\n",
        "\n",
        "            writer.add_scalar('data/test_top1_epoch', epoch_acc_cpu_top1, epoch)\n",
        "            writer.add_scalar('data/test_top3_epoch', epoch_acc_cpu_top3, epoch)\n",
        "            writer.add_scalar('data/epoch_acc_cpu_top1_MR', epoch_acc_cpu_top1_MR, epoch)\n",
        "            writer.add_scalar('data/epoch_acc_cpu_top3_MR', epoch_acc_cpu_top3_MR, epoch)\n",
        "            writer.add_scalar('data/epoch_acc_cpu_top1_IP', epoch_acc_cpu_top1_IP, epoch)\n",
        "            writer.add_scalar('data/epoch_acc_cpu_top3_IP', epoch_acc_cpu_top3_IP, epoch)\n",
        "            writer.add_scalar('data/epoch_acc_cpu_top1_MP', epoch_acc_cpu_top1_MP, epoch)\n",
        "            writer.add_scalar('data/epoch_acc_cpu_top3_MP', epoch_acc_cpu_top3_MP, epoch)\n",
        "\n",
        "            writer.flush()\n",
        "\n",
        "            print(\"[test] Epoch: {}/{} Loss: {} Intent Acc: {} Activity Acc: {} Pred Intent Acc: {} Pred Activity Acc: {}\"\n",
        "                  .format(epoch + 1, nEpochs, epoch_loss, epoch_acc_long, epoch_acc_mid, epoch_acc_long_pred, epoch_acc_mid_pred))\n",
        "\n",
        "            stop_time = timeit.default_timer()\n",
        "            print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
        "\n",
        "    print('testing', testing_acc_long, testing_acc_activity, testing_acc_long_pred, testing_acc_activity_pred)\n",
        "    print('plt plot')\n",
        "\n",
        "    wrtieresults_train(training_acc_long, training_acc_activity, training_acc_long_pred, training_acc_activity_pred)\n",
        "    wrtieresults(testing_acc_long, testing_acc_activity, testing_acc_long_pred, testing_acc_activity_pred)\n",
        "\n",
        "    wrtieresults_top_recog(testing_acc_top1, testing_acc_top3, testing_acc_top1_MR, testing_acc_top3_MR)\n",
        "    wrtieresults_top_pred(testing_acc_top1_IP, testing_acc_top3_IP, testing_acc_top1_MP, testing_acc_top3_MP)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "pred_horizon = np.arange(1,2)\n",
        "# pred_horizon = 0\n",
        "print(pred_horizon)\n",
        "for pre_h in pred_horizon:\n",
        "    print('pre_h', pre_h)\n",
        "    train_model(dataset=dataset, save_dir=save_dir, num_classes=num_classes, num_activities=num_activtites,  lr=lr,\n",
        "            num_epochs=nEpochs, save_epoch=snapshot, useTest=useTest, test_interval=nTestInterval, pred_horizon=pre_h)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "MSHARA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06f93c06a50241bf93efa790e26d7f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0075077355bb456f95c07ac181ce5825",
              "IPY_MODEL_faef3691c80748708915508c5de84068",
              "IPY_MODEL_626fd57856624c04af4f04c0f2d16667"
            ],
            "layout": "IPY_MODEL_e2e343d7ca12474a8cb130e70503da5a"
          }
        },
        "0075077355bb456f95c07ac181ce5825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48528cd23f684381ab922e8032c1cdb4",
            "placeholder": "​",
            "style": "IPY_MODEL_42f97674d39642358e9d04e36b5e3d0a",
            "value": "100%"
          }
        },
        "faef3691c80748708915508c5de84068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8757a349f202487198b32b792f8468a8",
            "max": 126162996,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9e2f2ec239b41f2bc2b0b2f88a39a1b",
            "value": 126162996
          }
        },
        "626fd57856624c04af4f04c0f2d16667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b7b1cff570a4d159f149ea955fe651c",
            "placeholder": "​",
            "style": "IPY_MODEL_4ce4eab49b7c430294ead3c30e32e2cc",
            "value": " 120M/120M [00:00&lt;00:00, 211MB/s]"
          }
        },
        "e2e343d7ca12474a8cb130e70503da5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48528cd23f684381ab922e8032c1cdb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42f97674d39642358e9d04e36b5e3d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8757a349f202487198b32b792f8468a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9e2f2ec239b41f2bc2b0b2f88a39a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b7b1cff570a4d159f149ea955fe651c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ce4eab49b7c430294ead3c30e32e2cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "534138a928d245df941cddd1aad8d064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc73b26bc04f44cdb5fdeae1097a5879",
              "IPY_MODEL_fb90682952df4523ba081ef4f9e2553b",
              "IPY_MODEL_1f7616eaf68d496f9acdb382e3a5152a"
            ],
            "layout": "IPY_MODEL_ca69bd5f6855439ca241ae10f94dc0f1"
          }
        },
        "cc73b26bc04f44cdb5fdeae1097a5879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f2d2a6ccbd4425681dfbc59b8b2026b",
            "placeholder": "​",
            "style": "IPY_MODEL_c30d0c37dcf64ac2b39086118da1289c",
            "value": "100%"
          }
        },
        "fb90682952df4523ba081ef4f9e2553b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d10a37f319bd48da81d59afa60db35e9",
            "max": 46827520,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4686fff6cfb44d0ae3e936355021420",
            "value": 46827520
          }
        },
        "1f7616eaf68d496f9acdb382e3a5152a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e304106727244844af3fba68933e1b7e",
            "placeholder": "​",
            "style": "IPY_MODEL_a5e3ce0206ff403f864889b2cfc2e24c",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 168MB/s]"
          }
        },
        "ca69bd5f6855439ca241ae10f94dc0f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f2d2a6ccbd4425681dfbc59b8b2026b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c30d0c37dcf64ac2b39086118da1289c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d10a37f319bd48da81d59afa60db35e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4686fff6cfb44d0ae3e936355021420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e304106727244844af3fba68933e1b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5e3ce0206ff403f864889b2cfc2e24c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}